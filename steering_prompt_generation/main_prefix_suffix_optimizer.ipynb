{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "14yYaxYnP3mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60821d65-ebca-497e-f7f3-4b61bf5e9bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda DTYPE: torch.bfloat16\n"
          ]
        }
      ],
      "source": [
        "import os, math, random, re\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 4738\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device / dtype\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = (\n",
        "    torch.bfloat16\n",
        "    if DEVICE == \"cuda\" and torch.cuda.is_bf16_supported()\n",
        "    else torch.float16\n",
        "    if DEVICE == \"cuda\"\n",
        "    else torch.float32\n",
        ")\n",
        "print(\"DEVICE:\", DEVICE, \"DTYPE:\", DTYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aI1X_uuJQOcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0a8801570b494e4387c7dcc1906c1914",
            "941613cc8aee4d9f835ab9fd6c8c2600",
            "1dd2508803064656aefe4e8e9c207aa1",
            "f6b45ba50d4d442dad688a9a94d7c9a5",
            "25cf76b7e574473ba8e3e04ef15567a5",
            "cb4eef1d70c5471abd9faf676fcfed7b",
            "679e77477a614557b705b051d6863213",
            "75c93e4b74d24181ba240fc784c87151",
            "6ea434173f1c4d11a6d02cc4f64282e3",
            "9b9440024de743f8a4af3e9762d6265c",
            "891f4ddb1cab408e8aa2f79e1290287d"
          ]
        },
        "outputId": "957c1892-c0b4-4951-f613-7f716174e8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a8801570b494e4387c7dcc1906c1914"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: google/gemma-2-2b-it\n",
            "attn_implementation: None\n",
            "vocab_size: 256000 d_model: 2304\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model_kwargs = dict(\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=None,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# IMPORTANT: we never train model weights; disabling grads prevents dtype/graph reuse issues\n",
        "for p in model.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "MODEL_DTYPE = next(model.parameters()).dtype   # should be torch.bfloat16 (or fp16 on some GPUs)\n",
        "DEVICE = next(model.parameters()).device\n",
        "\n",
        "embed_layer = model.get_input_embeddings()\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME)\n",
        "print(\"attn_implementation:\", getattr(model.config, \"attn_implementation\", None))\n",
        "print(\"vocab_size:\", model.config.vocab_size, \"d_model:\", model.config.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_LFb4TuQO25",
        "outputId": "3b4a17f2-9d1a-43f1-9c0e-79960f31f4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference tokens: 5 [27586, 1297, 1105, 19493, 235265] ...\n",
            "STEER_LEN: 5\n"
          ]
        }
      ],
      "source": [
        "P_REF = \"Talk only about cats.\"\n",
        "\n",
        "SUFFIXES = [\n",
        "  \"Write exactly two sentences explaining a simple process.\",\n",
        "  \"Output valid JSON with keys \\\"a\\\" and \\\"b\\\", values as integers.\",\n",
        "  \"Ask one clarifying question before providing any answer.\",\n",
        "  \"If unsure, say you are not sure and explain why.\",\n",
        "  \"Respond in exactly three bullet points, no extra text.\",\n",
        "  \"Write a haiku about uncertainty using exactly seventeen syllables.\",\n",
        "  \"Provide a concise definition using no commas.\",\n",
        "  \"Ask a clarifying question, then wait for confirmation.\",\n",
        "  \"Output a single word in all caps.\",\n",
        "  \"Explain a concept in a friendly, informal tone.\",\n",
        "  \"If the task seems ambiguous, refuse politely and state the ambiguity.\",\n",
        "  \"Produce valid JSON array with three strings.\",\n",
        "  \"Answer in exactly two sentences, each under ten words.\",\n",
        "  \"Ask one clarifying question before attempting a solution.\",\n",
        "  \"Write a short analogy involving tools and organization.\",\n",
        "  \"Respond using exactly five words.\",\n",
        "  \"If you lack confidence, say so and give a brief reason.\",\n",
        "  \"Create an acrostic spelling ORDER, one word per line.\",\n",
        "  \"Output YAML with keys alpha and beta only.\",\n",
        "  \"Explain a simple rule as if to a beginner.\",\n",
        "  \"Ask a clarifying question instead of answering directly.\",\n",
        "  \"Respond with exactly three numbered steps.\",\n",
        "  \"If information is insufficient, state uncertainty clearly.\",\n",
        "  \"Write a polite refusal to answer an unclear request.\",\n",
        "  \"Generate a single sentence without using the letter e.\",\n",
        "  \"Output JSON with a boolean field named \\\"ok\\\".\",\n",
        "  \"Explain the difference between speed and accuracy briefly.\",\n",
        "  \"Ask one clarifying question, no additional commentary.\",\n",
        "  \"Respond in a neutral, formal tone.\",\n",
        "  \"If unsure, explicitly say \\\"I don't know\\\" and why.\",\n",
        "  \"Write exactly four short lines forming a poem.\",\n",
        "  \"Provide an example using abstract placeholders only.\",\n",
        "  \"Return a CSV header line with three columns.\",\n",
        "  \"Explain a tradeoff in exactly fifteen words.\",\n",
        "  \"Ask a clarifying question before giving details.\",\n",
        "  \"Respond with a checklist of three items.\",\n",
        "  \"If the request is vague, refuse and explain succinctly.\",\n",
        "  \"Write a single paragraph under forty words.\",\n",
        "  \"Output valid JSON with nested object depth two.\",\n",
        "  \"Summarize a hypothetical scenario in one sentence.\",\n",
        "  \"Explain a simple algorithm conceptually, no code.\",\n",
        "  \"Ask one clarifying question, then stop.\",\n",
        "  \"Respond using only lowercase letters.\",\n",
        "  \"If unsure, explain what additional information is needed.\",\n",
        "  \"Create a title in title case, five words.\",\n",
        "  \"Provide a definition followed by one example.\",\n",
        "  \"Output exactly three emojis, no text.\",\n",
        "  \"Ask a clarifying question before proceeding.\",\n",
        "  \"Respond with exactly two clauses separated by a semicolon.\",\n",
        "  \"If you cannot answer confidently, say so briefly.\",\n",
        "  \"Write a rhyming couplet about balance.\",\n",
        "  \"Output JSON with keys \\\"status\\\" and \\\"reason\\\".\",\n",
        "  \"Explain a planning approach in three sentences.\",\n",
        "  \"Ask one clarifying question only.\",\n",
        "  \"Respond without using pronouns.\",\n",
        "  \"If uncertain, decline and request clarification.\",\n",
        "  \"Generate a slogan of exactly seven words.\",\n",
        "  \"Provide a pros and cons list, two each.\",\n",
        "  \"Output valid XML with a single root element.\",\n",
        "  \"Explain a concept using a metaphor.\",\n",
        "  \"Ask a clarifying question prior to answering.\",\n",
        "  \"Respond in exactly one sentence.\",\n",
        "  \"If information is missing, state uncertainty.\",\n",
        "  \"Write instructions using imperative verbs only.\",\n",
        "  \"Output JSON where values are empty strings.\",\n",
        "  \"Explain a mistake and how to avoid it.\",\n",
        "  \"Ask one clarifying question before any explanation.\",\n",
        "  \"Respond with exactly three short paragraphs.\",\n",
        "  \"If unsure, explain limitations of the answer.\",\n",
        "  \"Create a mnemonic using initial letters.\",\n",
        "  \"Provide a minimal example, no explanation.\",\n",
        "  \"Output a Markdown table with two columns, two rows.\",\n",
        "  \"Explain a decision-making principle briefly.\",\n",
        "  \"Ask a clarifying question and do not answer.\",\n",
        "  \"Respond using exactly ten words.\",\n",
        "  \"If the prompt is ambiguous, politely refuse.\",\n",
        "  \"Write a neutral summary in two sentences.\",\n",
        "  \"Output JSON with an array of two numbers.\",\n",
        "  \"Explain cause and effect succinctly.\",\n",
        "  \"Ask one clarifying question first.\",\n",
        "  \"Respond without punctuation marks.\",\n",
        "  \"If unsure, say so and suggest next steps.\",\n",
        "  \"Create a short checklist with checkboxes.\",\n",
        "  \"Provide a one-line definition.\",\n",
        "  \"Output exactly three hashtags.\",\n",
        "  \"Explain a workflow step-by-step, briefly.\",\n",
        "  \"Ask a clarifying question before solving.\",\n",
        "  \"Respond in a calm, professional tone.\",\n",
        "  \"If information is insufficient, decline respectfully.\",\n",
        "  \"Write a closing sentence for a report.\",\n",
        "  \"Output valid JSON with key \\\"result\\\" only.\"\n",
        "]\n",
        "\n",
        "max_n_suffixes = 10\n",
        "SUFFIXES = SUFFIXES[:max_n_suffixes]\n",
        "\n",
        "# Deterministic decoding so \"same output\" is meaningful\n",
        "GEN_KW = dict(\n",
        "    do_sample=False,\n",
        "    num_beams=1,\n",
        "    max_new_tokens=60,   # smaller while iterating; increase later\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "# Token-level matching metric for the *first K completion tokens*\n",
        "MATCH_FIRST_K = 50\n",
        "\n",
        "# Weight early completion tokens more heavily in CE loss (stabilizes exact-match)\n",
        "EARLY_K = 32\n",
        "EARLY_WEIGHT = 3.0\n",
        "\n",
        "# Steering length: currently match the reference length exactly\n",
        "P_REF_IDS = tokenizer(P_REF, add_special_tokens=False).input_ids\n",
        "STEER_LEN = len(P_REF_IDS)\n",
        "print(\"Reference tokens:\", len(P_REF_IDS), P_REF_IDS[:10], \"...\")\n",
        "print(\"STEER_LEN:\", STEER_LEN)\n",
        "\n",
        "# Chat-template scaffolding (recommended for instruction-tuned Gemma)\n",
        "USE_CHAT_TEMPLATE = True\n",
        "SEP_TEXT = \"\\n\\n\"\n",
        "\n",
        "def _find_subseq(hay: List[int], needle: List[int]) -> int:\n",
        "    \"\"\"Return start index of needle in hay, or -1.\"\"\"\n",
        "    if not needle:\n",
        "        return -1\n",
        "    for i in range(0, len(hay) - len(needle) + 1):\n",
        "        if hay[i : i + len(needle)] == needle:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "@dataclass\n",
        "class ChatScaffold:\n",
        "    # For generation (user message -> assistant generation prompt)\n",
        "    gen_pre: List[int]\n",
        "    gen_post: List[int]\n",
        "    # For teacher forcing (user message + assistant message content)\n",
        "    tf_pre: List[int]\n",
        "    tf_between: List[int]\n",
        "    tf_post: List[int]\n",
        "\n",
        "def build_chat_scaffold(tok: AutoTokenizer) -> ChatScaffold:\n",
        "    \"\"\"\n",
        "    Build token-level scaffolds using tokenizer.apply_chat_template and placeholders,\n",
        "    so we can splice token IDs reliably without re-tokenizing whole prompts.\n",
        "    \"\"\"\n",
        "    if not getattr(tok, \"apply_chat_template\", None):\n",
        "        raise RuntimeError(\"Tokenizer has no chat template; set USE_CHAT_TEMPLATE=False or use a compatible tokenizer.\")\n",
        "\n",
        "    user_ph = \"<<<USER_CONTENT_PLACEHOLDER>>>\"\n",
        "    asst_ph = \"<<<ASSISTANT_CONTENT_PLACEHOLDER>>>\"\n",
        "\n",
        "    user_ph_ids = tok(user_ph, add_special_tokens=False).input_ids\n",
        "    asst_ph_ids = tok(asst_ph, add_special_tokens=False).input_ids\n",
        "\n",
        "    # Generation scaffold: user(content) then assistant prompt\n",
        "    gen_ids = tok.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": user_ph}],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "    if isinstance(gen_ids, torch.Tensor):\n",
        "        gen_ids = gen_ids.tolist()\n",
        "    u0 = _find_subseq(gen_ids, user_ph_ids)\n",
        "    if u0 < 0:\n",
        "        raise RuntimeError(\"Couldn't locate user placeholder tokens inside gen chat template.\")\n",
        "    gen_pre = gen_ids[:u0]\n",
        "    gen_post = gen_ids[u0 + len(user_ph_ids) :]\n",
        "\n",
        "    # Teacher-forcing scaffold: user(content) + assistant(content)\n",
        "    tf_ids = tok.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": user_ph}, {\"role\": \"assistant\", \"content\": asst_ph}],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "    if isinstance(tf_ids, torch.Tensor):\n",
        "        tf_ids = tf_ids.tolist()\n",
        "    u1 = _find_subseq(tf_ids, user_ph_ids)\n",
        "    a1 = _find_subseq(tf_ids, asst_ph_ids)\n",
        "    if u1 < 0 or a1 < 0 or a1 <= u1:\n",
        "        raise RuntimeError(\"Couldn't locate placeholders inside teacher-forcing chat template.\")\n",
        "    tf_pre = tf_ids[:u1]\n",
        "    tf_between = tf_ids[u1 + len(user_ph_ids) : a1]\n",
        "    tf_post = tf_ids[a1 + len(asst_ph_ids) :]\n",
        "\n",
        "    return ChatScaffold(gen_pre=gen_pre, gen_post=gen_post, tf_pre=tf_pre, tf_between=tf_between, tf_post=tf_post)\n",
        "\n",
        "if USE_CHAT_TEMPLATE:\n",
        "    CHAT = build_chat_scaffold(tokenizer)\n",
        "    SEP_IDS = tokenizer(SEP_TEXT, add_special_tokens=False).input_ids\n",
        "else:\n",
        "    CHAT = None\n",
        "    SEP_IDS = tokenizer(SEP_TEXT, add_special_tokens=False).input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1WBPV48VQO5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb8bdb2-e667-44d5-d063-492576ba86c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUFFIX: Write exactly two sentences explaining a simple process.\n",
            "Y_REF: Cats groom themselves by licking their fur, removing dirt and loose hairs.  This process helps keep their coat healthy and shiny. \n",
            " ...\n",
            "\n",
            "SUFFIX: Output valid JSON with keys \"a\" and \"b\", values as integers.\n",
            "Y_REF: ```json\n",
            "{\n",
            "  \"a\": 3,\n",
            "  \"b\": 17\n",
            "}\n",
            "``` ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def greedy_generate_ids(prefix_ids: List[int], suffix: str, **gen_kw) -> str:\n",
        "    \"\"\"\n",
        "    Greedy-generate assistant completion given a discrete prefix token list + suffix text.\n",
        "    Uses chat template scaffolding if enabled.\n",
        "    Returns completion text (decoded generated tokens after the prompt).\n",
        "    \"\"\"\n",
        "    sfx_ids = tokenizer(suffix, add_special_tokens=False).input_ids\n",
        "\n",
        "    if USE_CHAT_TEMPLATE:\n",
        "        prompt_ids = CHAT.gen_pre + prefix_ids + SEP_IDS + sfx_ids + CHAT.gen_post\n",
        "    else:\n",
        "        prompt_ids = prefix_ids + SEP_IDS + sfx_ids\n",
        "\n",
        "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=DEVICE)\n",
        "    attn = torch.ones_like(input_ids, dtype=torch.long)\n",
        "\n",
        "    out = model.generate(input_ids=input_ids, attention_mask=attn, **gen_kw)\n",
        "    gen_ids = out[0].tolist()[len(prompt_ids):]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_generate_text(prefix_text: str, suffix: str, **gen_kw) -> str:\n",
        "    prefix_ids = tokenizer(prefix_text, add_special_tokens=False).input_ids\n",
        "    return greedy_generate_ids(prefix_ids, suffix, **gen_kw)\n",
        "\n",
        "# Build deterministic reference outputs\n",
        "Y_REF: Dict[str, str] = {}\n",
        "for sfx in SUFFIXES:\n",
        "    Y_REF[sfx] = greedy_generate_text(P_REF, sfx, **GEN_KW)\n",
        "\n",
        "for sfx in SUFFIXES[:2]:\n",
        "    print(\"SUFFIX:\", sfx)\n",
        "    print(\"Y_REF:\", Y_REF[sfx][:220], \"...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fHCa2rxUQO82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8319f5-7b55-4c99-807d-6015eacb22d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 10 Max suffix+completion length: 72\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class Example:\n",
        "    suffix: str\n",
        "    ref_completion: str\n",
        "    suffix_ids: torch.Tensor        # [Ts]\n",
        "    completion_ids: torch.Tensor    # [Tc]\n",
        "\n",
        "def build_example(suffix: str, completion: str) -> Example:\n",
        "    sfx_ids = torch.tensor(\n",
        "        tokenizer(suffix, add_special_tokens=False).input_ids,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    comp_ids = torch.tensor(\n",
        "        tokenizer(completion, add_special_tokens=False).input_ids,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    return Example(suffix=suffix, ref_completion=completion, suffix_ids=sfx_ids, completion_ids=comp_ids)\n",
        "\n",
        "examples: List[Example] = [build_example(sfx, Y_REF[sfx]) for sfx in SUFFIXES]\n",
        "max_len = max((ex.suffix_ids.numel() + ex.completion_ids.numel()) for ex in examples)\n",
        "print(\"Num examples:\", len(examples), \"Max suffix+completion length:\", max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HcU3tAjqQO_Q"
      },
      "outputs": [],
      "source": [
        "def pad_1d(seqs: List[torch.Tensor], pad_value: int) -> torch.Tensor:\n",
        "    max_len = max(x.numel() for x in seqs)\n",
        "    out = torch.full((len(seqs), max_len), pad_value, dtype=seqs[0].dtype, device=seqs[0].device)\n",
        "    for i, x in enumerate(seqs):\n",
        "        out[i, : x.numel()] = x\n",
        "    return out\n",
        "\n",
        "def make_batch_with_prefix_ids(prefix_ids: torch.Tensor, batch: List[Example]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Build padded batch for teacher-forcing:\n",
        "      input_ids = [chat_pre] + prefix_ids + [SEP] + suffix_ids + [chat_between] + completion_ids + [chat_post]\n",
        "    labels are -100 everywhere except completion token positions.\n",
        "    loss_weights are 0 on ignored positions, and (EARLY_WEIGHT or 1.0) on completion positions\n",
        "    (heavier for first EARLY_K completion tokens).\n",
        "    \"\"\"\n",
        "    device = DEVICE\n",
        "    prefix_ids = prefix_ids.to(device)\n",
        "\n",
        "    pre = torch.tensor(CHAT.tf_pre if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=device)\n",
        "    between = torch.tensor(CHAT.tf_between if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=device)\n",
        "    post = torch.tensor(CHAT.tf_post if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=device)\n",
        "    sep = torch.tensor(SEP_IDS, dtype=torch.long, device=device)\n",
        "\n",
        "    input_ids_list, labels_list, attn_list, w_list = [], [], [], []\n",
        "\n",
        "    for ex in batch:\n",
        "        sfx = ex.suffix_ids.to(device)\n",
        "        comp = ex.completion_ids.to(device)\n",
        "\n",
        "        ids = torch.cat([pre, prefix_ids, sep, sfx, between, comp, post], dim=0)\n",
        "\n",
        "        labels = torch.full_like(ids, -100)\n",
        "        weights = torch.zeros_like(ids, dtype=torch.float)\n",
        "\n",
        "        # completion starts after: pre + prefix + sep + suffix + between\n",
        "        comp_start = pre.numel() + prefix_ids.numel() + sep.numel() + sfx.numel() + between.numel()\n",
        "        labels[comp_start : comp_start + comp.numel()] = comp\n",
        "\n",
        "        # early-token weighting\n",
        "        for i in range(comp.numel()):\n",
        "            w = EARLY_WEIGHT if i < EARLY_K else 1.0\n",
        "            weights[comp_start + i] = w\n",
        "\n",
        "        attn = torch.ones_like(ids, dtype=torch.long)\n",
        "\n",
        "        input_ids_list.append(ids)\n",
        "        labels_list.append(labels)\n",
        "        attn_list.append(attn)\n",
        "        w_list.append(weights)\n",
        "\n",
        "    input_ids = pad_1d(input_ids_list, pad_value=tokenizer.pad_token_id)\n",
        "    labels = pad_1d(labels_list, pad_value=-100)\n",
        "    attention_mask = pad_1d(attn_list, pad_value=0)\n",
        "    loss_weights = pad_1d(w_list, pad_value=0).to(torch.float)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask, \"loss_weights\": loss_weights}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QTNfc6joQPB3"
      },
      "outputs": [],
      "source": [
        "def weighted_teacher_forced_ce(prefix_ids: torch.Tensor, batch: List[Example]) -> torch.Tensor:\n",
        "    b = make_batch_with_prefix_ids(prefix_ids, batch)\n",
        "    out = model(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n",
        "    logits = out.logits[:, :-1, :].contiguous()\n",
        "    labels = b[\"labels\"][:, 1:].contiguous()\n",
        "    weights = b[\"loss_weights\"][:, 1:].contiguous()\n",
        "\n",
        "    # per-token loss\n",
        "    per = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        labels.view(-1),\n",
        "        ignore_index=-100,\n",
        "        reduction=\"none\",\n",
        "    ).view(labels.shape)\n",
        "\n",
        "    mask = (labels != -100).float()\n",
        "    denom = (weights * mask).sum().clamp_min(1.0)\n",
        "    return (per * weights * mask).sum() / denom\n",
        "\n",
        "@torch.no_grad()\n",
        "def batch_ce(prefix_ids: List[int], batch: List[Example]) -> float:\n",
        "    p = torch.tensor(prefix_ids, dtype=torch.long, device=DEVICE)\n",
        "    return float(weighted_teacher_forced_ce(p, batch).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XpRuiG1iQPEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "826aaaed-5271-4f8f-9b03-fbf1a6d4f4ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptConstraints(ref_ids=[27586, 1297, 1105, 19493, 235265], max_token_overlap_frac=0.25, min_edit_distance=5, forbid_ref_substring=True, require_printable=True, ascii_only=True, max_len_chars=80, forbid_phrases=['Talk only about cats.', 'Talk', 'only', 'about', 'cats.'], phrase_case_insensitive=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def levenshtein(a: List[int], b: List[int]) -> int:\n",
        "    n, m = len(a), len(b)\n",
        "    dp = list(range(m + 1))\n",
        "    for i in range(1, n + 1):\n",
        "        prev = dp[0]\n",
        "        dp[0] = i\n",
        "        for j in range(1, m + 1):\n",
        "            cur = dp[j]\n",
        "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
        "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)\n",
        "            prev = cur\n",
        "    return dp[m]\n",
        "\n",
        "@dataclass\n",
        "class PromptConstraints:\n",
        "    ref_ids: List[int]\n",
        "    max_token_overlap_frac: float = 0.25\n",
        "    min_edit_distance: int = 4\n",
        "    forbid_ref_substring: bool = True\n",
        "    require_printable: bool = True\n",
        "    ascii_only: bool = True\n",
        "    max_len_chars: int = 80\n",
        "    # NEW: phrase-level bans\n",
        "    forbid_phrases: Optional[List[str]] = None\n",
        "    phrase_case_insensitive: bool = True\n",
        "\n",
        "def constraints_ok(prefix_ids: List[int], cons: PromptConstraints) -> bool:\n",
        "    ref_ids = cons.ref_ids\n",
        "\n",
        "    # overlap\n",
        "    if ref_ids:\n",
        "        overlap = len(set(prefix_ids) & set(ref_ids))\n",
        "        if overlap / max(1, len(set(ref_ids))) > cons.max_token_overlap_frac:\n",
        "            return False\n",
        "\n",
        "    # edit distance\n",
        "    if levenshtein(prefix_ids, ref_ids) < cons.min_edit_distance:\n",
        "        return False\n",
        "\n",
        "    # decoded string once\n",
        "    s = tokenizer.decode(prefix_ids, skip_special_tokens=True)\n",
        "\n",
        "    # decoded substring (exact ref string)\n",
        "    if cons.forbid_ref_substring and ref_ids:\n",
        "        b = tokenizer.decode(ref_ids, skip_special_tokens=True)\n",
        "        if b.strip() and (b.strip() in s):\n",
        "            return False\n",
        "\n",
        "    # NEW: phrase-level bans (handles “Talk only about cats...” etc)\n",
        "    if cons.forbid_phrases:\n",
        "        def _norm(text: str) -> str:\n",
        "            if cons.phrase_case_insensitive:\n",
        "                text = text.lower()\n",
        "            text = re.sub(r\"\\s+\", \" \", text)\n",
        "            return text.strip()\n",
        "\n",
        "        s_norm = _norm(s)\n",
        "        for ph in cons.forbid_phrases:\n",
        "            if not ph:\n",
        "                continue\n",
        "            if _norm(ph) in s_norm:\n",
        "                return False\n",
        "\n",
        "    # readability\n",
        "    if not s.strip():\n",
        "        return False\n",
        "    if cons.require_printable and any(not ch.isprintable() for ch in s):\n",
        "        return False\n",
        "    if cons.ascii_only and any(ord(ch) > 127 for ch in s):\n",
        "        return False\n",
        "    if len(s) > cons.max_len_chars:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "constraints = PromptConstraints(\n",
        "    ref_ids=P_REF_IDS,\n",
        "    min_edit_distance=max(4, len(P_REF_IDS)),\n",
        "    # Ban the teacher prompt text itself (you can add more phrases here)\n",
        "    forbid_phrases=[P_REF, *P_REF.split(\" \")],\n",
        ")\n",
        "\n",
        "constraints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_banned_token_ids(phrases: List[str]) -> set:\n",
        "    \"\"\"\n",
        "    Build a set of token IDs that should never be used in the steering prefix,\n",
        "    by tokenizing several simple variants of each phrase.\n",
        "    \"\"\"\n",
        "    variants = []\n",
        "    for p in phrases:\n",
        "        if not p:\n",
        "            continue\n",
        "        variants.extend([\n",
        "            p,\n",
        "            \" \" + p,\n",
        "            \"\\n\" + p,\n",
        "            \"\\n\\n\" + p,\n",
        "            p.lower(),\n",
        "            p.upper(),\n",
        "        ])\n",
        "    banned = set()\n",
        "    for v in variants:\n",
        "        ids = tokenizer(v, add_special_tokens=False).input_ids\n",
        "        banned.update(ids)\n",
        "    return banned"
      ],
      "metadata": {
        "id": "-iKLiwWU6PIm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "goTqjTdlQPHd"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "class SoftSteeringPrompt(nn.Module):\n",
        "    \"\"\"\n",
        "    Soft steering prompt parameterized as per-position logits over the vocab.\n",
        "\n",
        "    Forward:\n",
        "      - Applies softmax over logits to get a token distribution per position.\n",
        "      - Multiplies by the (frozen) embedding matrix to get a convex combination\n",
        "        of *valid token embeddings* at each position.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        prompt_len: int,\n",
        "        vocab_size: int,\n",
        "        init_from_ref_ids: Optional[List[int]] = None,\n",
        "        init_std: float = 0.5,\n",
        "        *,\n",
        "        device=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if device is None:\n",
        "            device = DEVICE\n",
        "\n",
        "        self.prompt_len = prompt_len\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # We keep logits in float32 for numerical stability.\n",
        "        self.logits = nn.Parameter(\n",
        "            torch.zeros(prompt_len, vocab_size, device=device, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "        # Initialize around the reference tokens if provided, otherwise random.\n",
        "        with torch.no_grad():\n",
        "            if init_from_ref_ids:\n",
        "                ids = list(init_from_ref_ids)\n",
        "                if len(ids) < prompt_len:\n",
        "                    reps = math.ceil(prompt_len / len(ids))\n",
        "                    ids = (ids * reps)[:prompt_len]\n",
        "                else:\n",
        "                    ids = ids[:prompt_len]\n",
        "\n",
        "                # Start with small noise, then give a positive bias to the ref token at each pos.\n",
        "                self.logits.data.normal_(mean=0.0, std=init_std)\n",
        "                for i, tok in enumerate(ids):\n",
        "                    if 0 <= tok < vocab_size:\n",
        "                        self.logits.data[i, tok] += 3.0\n",
        "            else:\n",
        "                self.logits.data.normal_(mean=0.0, std=init_std)\n",
        "\n",
        "    def forward(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns soft prompt embeddings of shape [L, d_model] in MODEL_DTYPE,\n",
        "        each row a convex combination of token embeddings.\n",
        "        \"\"\"\n",
        "        # [L, V] float32\n",
        "        logits = self.logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Embedding matrix: [V, d]; cast to float32 for matmul, then back to MODEL_DTYPE\n",
        "        E = embed_layer.weight.to(torch.float32)\n",
        "        emb = probs @ E  # [L, d] float32\n",
        "\n",
        "        return emb.to(dtype=MODEL_DTYPE, device=DEVICE)\n",
        "\n",
        "\n",
        "def train_soft_prompt(\n",
        "    prompt_len: int,\n",
        "    batch_examples: List[Example],\n",
        "    steps: int = 200,\n",
        "    lr: float = 2e-2,\n",
        "    batch_size: int = 4,\n",
        "    init_from_ref: Optional[List[int]] = None,\n",
        "    repulsion_weight: float = 0.03,\n",
        ") -> SoftSteeringPrompt:\n",
        "    \"\"\"\n",
        "    Optimize a *token-mixture* soft prompt so that, under suffix prompts, the model\n",
        "    assigns high likelihood to the reference completions (weighted teacher-forced CE).\n",
        "\n",
        "    The soft embeddings at each position are convex combinations of token embeddings,\n",
        "    which keeps them close to valid token embeddings (reduces soft→discrete mismatch).\n",
        "    \"\"\"\n",
        "    vocab_size = embed_layer.weight.shape[0]\n",
        "    sp = SoftSteeringPrompt(\n",
        "        prompt_len=prompt_len,\n",
        "        vocab_size=vocab_size,\n",
        "        init_from_ref_ids=init_from_ref,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "    # We optimize logits (float32) only.\n",
        "    opt = torch.optim.AdamW([sp.logits], lr=lr, betas=(0.9, 0.98), weight_decay=0.0)\n",
        "\n",
        "    # fixed tensors for chat scaffold\n",
        "    pre = torch.tensor(CHAT.tf_pre if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=DEVICE)\n",
        "    between = torch.tensor(CHAT.tf_between if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=DEVICE)\n",
        "    post = torch.tensor(CHAT.tf_post if USE_CHAT_TEMPLATE else [], dtype=torch.long, device=DEVICE)\n",
        "    sep = torch.tensor(SEP_IDS, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    # Cache scaffold embeddings, DETACHED so they don't keep a graph across steps\n",
        "    with torch.no_grad():\n",
        "        pre_emb = embed_layer(pre).unsqueeze(0) if pre.numel() else None\n",
        "        between_emb = embed_layer(between).unsqueeze(0) if between.numel() else None\n",
        "        post_emb = embed_layer(post).unsqueeze(0) if post.numel() else None\n",
        "        sep_emb = embed_layer(sep).unsqueeze(0)\n",
        "\n",
        "        ref_ids_for_mean = tokenizer(P_REF, add_special_tokens=False).input_ids\n",
        "        ref_mean = embed_layer(torch.tensor(ref_ids_for_mean, device=DEVICE)).mean(dim=0, keepdim=True)  # [1,d]\n",
        "\n",
        "    ref_mean = ref_mean.to(dtype=MODEL_DTYPE)\n",
        "\n",
        "    for step in range(steps):\n",
        "        batch = random.sample(batch_examples, k=min(batch_size, len(batch_examples)))\n",
        "\n",
        "        emb_seqs = []\n",
        "        labels_seqs = []\n",
        "        attn_seqs = []\n",
        "        w_seqs = []\n",
        "\n",
        "        for ex in batch:\n",
        "            sfx = ex.suffix_ids.to(DEVICE)\n",
        "            comp = ex.completion_ids.to(DEVICE)\n",
        "\n",
        "            ids = torch.cat([sep, sfx, between, comp, post], dim=0)\n",
        "            with torch.no_grad():\n",
        "                emb = embed_layer(ids).unsqueeze(0)  # [1,T,d] detached by no_grad\n",
        "            emb = emb.to(dtype=MODEL_DTYPE)\n",
        "\n",
        "            full_len = (pre.numel() + prompt_len + ids.numel())\n",
        "            labels = torch.full((full_len,), -100, dtype=torch.long, device=DEVICE)\n",
        "            weights = torch.zeros((full_len,), dtype=torch.float, device=DEVICE)\n",
        "\n",
        "            comp_start = pre.numel() + prompt_len + sep.numel() + sfx.numel() + between.numel()\n",
        "            labels[comp_start : comp_start + comp.numel()] = comp\n",
        "            for i in range(comp.numel()):\n",
        "                weights[comp_start + i] = (EARLY_WEIGHT if i < EARLY_K else 1.0)\n",
        "\n",
        "            attn = torch.ones((full_len,), dtype=torch.long, device=DEVICE)\n",
        "\n",
        "            emb_seqs.append(emb)\n",
        "            labels_seqs.append(labels)\n",
        "            attn_seqs.append(attn)\n",
        "            w_seqs.append(weights)\n",
        "\n",
        "        max_full = max(x.numel() for x in labels_seqs)\n",
        "        B = len(batch)\n",
        "        d = model.config.hidden_size\n",
        "\n",
        "        # IMPORTANT: ensure inputs_embeds matches model dtype\n",
        "        inputs_embeds = torch.zeros((B, max_full, d), dtype=MODEL_DTYPE, device=DEVICE)\n",
        "        labels = torch.full((B, max_full), -100, dtype=torch.long, device=DEVICE)\n",
        "        attn = torch.zeros((B, max_full), dtype=torch.long, device=DEVICE)\n",
        "        weights = torch.zeros((B, max_full), dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # soft prompt embeddings from logits (convex combination of token embeddings)\n",
        "        soft = sp().unsqueeze(0)  # [1,L,d] in MODEL_DTYPE\n",
        "        for i in range(B):\n",
        "            L_i = labels_seqs[i].numel()\n",
        "\n",
        "            off = 0\n",
        "            if pre_emb is not None:\n",
        "                inputs_embeds[i, off : off + pre_emb.shape[1], :] = pre_emb[0].to(dtype=MODEL_DTYPE)\n",
        "                off += pre_emb.shape[1]\n",
        "\n",
        "            inputs_embeds[i, off : off + prompt_len, :] = soft[0]\n",
        "            off += prompt_len\n",
        "\n",
        "            rest = emb_seqs[i]\n",
        "            inputs_embeds[i, off : off + rest.shape[1], :] = rest[0]\n",
        "            off += rest.shape[1]\n",
        "\n",
        "            labels[i, :L_i] = labels_seqs[i]\n",
        "            attn[i, :L_i] = attn_seqs[i]\n",
        "            weights[i, :L_i] = w_seqs[i]\n",
        "\n",
        "        # Defensive cast before forward\n",
        "        inputs_embeds = inputs_embeds.to(dtype=MODEL_DTYPE, device=DEVICE)\n",
        "\n",
        "        out = model(inputs_embeds=inputs_embeds, attention_mask=attn)\n",
        "        logits = out.logits[:, :-1, :]\n",
        "        lab = labels[:, 1:]\n",
        "        w = weights[:, 1:]\n",
        "        per = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            lab.reshape(-1),\n",
        "            ignore_index=-100,\n",
        "            reduction=\"none\",\n",
        "        ).view_as(lab)\n",
        "\n",
        "        mask = (lab != -100).float()\n",
        "        ce = (per * w * mask).sum() / (w * mask).sum().clamp_min(1.0)\n",
        "\n",
        "        # repulsion from reference mean (keep small; constraints do the real \"different\")\n",
        "        soft_mean = soft.mean(dim=1)  # [1,d]\n",
        "        rep = F.cosine_similarity(soft_mean, ref_mean, dim=-1).mean()\n",
        "        loss = ce + repulsion_weight * rep\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if step % 25 == 0 or step == steps - 1:\n",
        "            print(f\"[soft step {step:4d}] loss={loss.item():.4f}  ce={ce.item():.7f}\")\n",
        "\n",
        "    return sp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-bm7ThIUQPKN"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# %%\n",
        "@torch.no_grad()\n",
        "def project_soft_to_tokens(\n",
        "    logits: torch.Tensor,\n",
        "    banned_token_ids: Optional[set] = None,\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Project per-position logits over the vocab to a discrete token ID per position.\n",
        "\n",
        "    logits: [L, V] float tensor (e.g. from SoftSteeringPrompt.logits)\n",
        "    Returns: List[int] of length L, best token per position, skipping banned_token_ids.\n",
        "    \"\"\"\n",
        "    if banned_token_ids is None:\n",
        "        banned_token_ids = set()\n",
        "\n",
        "    L, V = logits.shape\n",
        "    out_ids: List[int] = []\n",
        "\n",
        "    for i in range(L):\n",
        "        row = logits[i].clone()\n",
        "        # Mask out banned IDs for this position\n",
        "        for t in banned_token_ids:\n",
        "            if 0 <= t < V:\n",
        "                row[t] = float(\"-inf\")\n",
        "        best_id = int(torch.argmax(row).item())\n",
        "        out_ids.append(best_id)\n",
        "\n",
        "    return out_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mh8_5kY3QPNG"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_discrete_prefix(prefix_ids: List[int], suffixes: List[str], y_ref: Dict[str, str]) -> Dict[str, float]:\n",
        "    batch = [build_example(s, y_ref[s]) for s in suffixes]\n",
        "    p = torch.tensor(prefix_ids, dtype=torch.long, device=DEVICE)\n",
        "    ce = float(weighted_teacher_forced_ce(p, batch).item())\n",
        "\n",
        "    exact = 0\n",
        "    exact_k = 0\n",
        "    tok_match_sum = 0\n",
        "    tok_total = 0\n",
        "    tok_match_k_sum = 0\n",
        "    tok_total_k = 0\n",
        "\n",
        "    for s in suffixes:\n",
        "        cand = greedy_generate_ids(prefix_ids, s, **GEN_KW)\n",
        "        ref = y_ref[s]\n",
        "        if cand == ref:\n",
        "            exact += 1\n",
        "\n",
        "        c_ids = tokenizer(cand, add_special_tokens=False).input_ids\n",
        "        r_ids = tokenizer(ref, add_special_tokens=False).input_ids\n",
        "\n",
        "        L = min(len(c_ids), len(r_ids))\n",
        "        tok_match_sum += sum(1 for i in range(L) if c_ids[i] == r_ids[i])\n",
        "        tok_total += max(1, len(r_ids))\n",
        "\n",
        "        K = min(MATCH_FIRST_K, len(r_ids), len(c_ids))\n",
        "        tok_match_k_sum += sum(1 for i in range(K) if c_ids[i] == r_ids[i])\n",
        "        tok_total_k += max(1, min(MATCH_FIRST_K, len(r_ids)))\n",
        "\n",
        "        if c_ids[:K] == r_ids[:K] and K > 0:\n",
        "            exact_k += 1\n",
        "\n",
        "    n = max(1, len(suffixes))\n",
        "    return {\n",
        "        \"ce_loss\": ce,\n",
        "        \"exact_match_rate\": exact / n,\n",
        "        \"exact_match_rate_firstk\": exact_k / n,\n",
        "        \"token_match_rate\": tok_match_sum / max(1, tok_total),\n",
        "        \"token_match_rate_firstk\": tok_match_k_sum / max(1, tok_total_k),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DY4CpxghQPQP"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def compute_prefix_grads(prefix_ids: List[int], batch: List[Example]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute average gradient w.r.t. prefix *position embeddings* for the current discrete prefix.\n",
        "    Returns [L,d].\n",
        "    \"\"\"\n",
        "    device = DEVICE\n",
        "    p = torch.tensor(prefix_ids, dtype=torch.long, device=device)\n",
        "    b = make_batch_with_prefix_ids(p, batch)\n",
        "\n",
        "    # Embed full sequence and backprop through embeddings directly\n",
        "    inputs_embeds = embed_layer(b[\"input_ids\"]).detach().to(dtype=MODEL_DTYPE)\n",
        "    inputs_embeds.requires_grad_(True)\n",
        "\n",
        "    out = model(inputs_embeds=inputs_embeds, attention_mask=b[\"attention_mask\"])\n",
        "    logits = out.logits[:, :-1, :]\n",
        "    labels = b[\"labels\"][:, 1:]\n",
        "    weights = b[\"loss_weights\"][:, 1:]\n",
        "\n",
        "    per = F.cross_entropy(\n",
        "        logits.reshape(-1, logits.size(-1)),\n",
        "        labels.reshape(-1),\n",
        "        ignore_index=-100,\n",
        "        reduction=\"none\",\n",
        "    ).view_as(labels)\n",
        "\n",
        "    mask = (labels != -100).float()\n",
        "    loss = (per * weights * mask).sum() / (weights * mask).sum().clamp_min(1.0)\n",
        "\n",
        "    # IMPORTANT: clear any existing grads\n",
        "    if inputs_embeds.grad is not None:\n",
        "        inputs_embeds.grad.zero_()\n",
        "    loss.backward()\n",
        "\n",
        "    # Prefix starts after chat tf_pre\n",
        "    pre_len = len(CHAT.tf_pre) if USE_CHAT_TEMPLATE else 0\n",
        "    L = len(prefix_ids)\n",
        "    g = inputs_embeds.grad[:, pre_len : pre_len + L, :].mean(dim=0)  # [L,d]\n",
        "    return g.detach()\n",
        "\n",
        "@torch.no_grad()\n",
        "def hotflip_candidates(\n",
        "    grad: torch.Tensor,                 # [d]\n",
        "    current_id: int,\n",
        "    top_k: int = 32,\n",
        "    banned_token_ids: Optional[set] = None,\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    HotFlip: choose token replacements that most reduce loss (approx) via grad · (e_new - e_cur).\n",
        "    \"\"\"\n",
        "    if banned_token_ids is None:\n",
        "        banned_token_ids = set()\n",
        "\n",
        "    w = embed_layer.weight.detach()  # [V,d]\n",
        "    # use float32 scoring for stability\n",
        "    g = grad.float()\n",
        "    w_f = w.float()\n",
        "\n",
        "    scores = torch.mv(w_f, g)  # [V]\n",
        "    scores[current_id] = float(\"inf\")\n",
        "    for t in banned_token_ids:\n",
        "        if 0 <= t < scores.numel():\n",
        "            scores[t] = float(\"inf\")\n",
        "\n",
        "    _, idx = torch.topk(scores, k=top_k, largest=False)\n",
        "    return idx.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a1BOjKgUQPU6"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def refine_discrete_prefix(\n",
        "    init_prefix_ids: List[int],\n",
        "    batch_examples: List[Example],\n",
        "    cons: PromptConstraints,\n",
        "    passes: int = 6,\n",
        "    hotflip_top_k: int = 64,\n",
        "    eval_top_k_per_pos: int = 12,\n",
        "    banned_token_ids: Optional[set] = None,\n",
        ") -> List[int]:\n",
        "    if banned_token_ids is None:\n",
        "        banned_token_ids = set()\n",
        "\n",
        "    prefix = init_prefix_ids[:]\n",
        "    best = batch_ce(prefix, batch_examples)\n",
        "    print(\"Init CE:\", best)\n",
        "\n",
        "    for p_i in range(passes):\n",
        "        improved = False\n",
        "        grads = compute_prefix_grads(prefix, batch_examples)  # [L,d]\n",
        "\n",
        "        for pos in range(len(prefix)):\n",
        "            cur_id = prefix[pos]\n",
        "            cand_ids = hotflip_candidates(grads[pos], cur_id, top_k=hotflip_top_k, banned_token_ids=banned_token_ids)\n",
        "\n",
        "            shortlist = cand_ids[:eval_top_k_per_pos]\n",
        "            local_best = best\n",
        "            local_best_tok = cur_id\n",
        "\n",
        "            for tok in shortlist:\n",
        "                trial = prefix[:]\n",
        "                trial[pos] = tok\n",
        "                if not constraints_ok(trial, cons):\n",
        "                    continue\n",
        "                loss = batch_ce(trial, batch_examples)\n",
        "                if loss < local_best:\n",
        "                    local_best = loss\n",
        "                    local_best_tok = tok\n",
        "\n",
        "            if local_best_tok != cur_id:\n",
        "                prefix[pos] = local_best_tok\n",
        "                best = local_best\n",
        "                improved = True\n",
        "                print(f\"[pass {p_i+1}/{passes}] pos {pos:2d} -> loss {best:.7f}\")\n",
        "\n",
        "        if not improved:\n",
        "            print(f\"[pass {p_i+1}/{passes}] no improvement; stopping.\")\n",
        "            break\n",
        "\n",
        "    return prefix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VxcomPEaQkft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6efe9828-5599-4420-d31a-496e0c5b45ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[soft step    0] loss=1.0149  ce=1.0251418\n",
            "[soft step   25] loss=1.2324  ce=1.2425153\n",
            "[soft step   50] loss=1.3405  ce=1.3506116\n",
            "[soft step   75] loss=0.9909  ce=1.0009747\n",
            "[soft step  100] loss=1.1603  ce=1.1703049\n",
            "[soft step  125] loss=0.8686  ce=0.8786584\n",
            "[soft step  150] loss=0.7446  ce=0.7546949\n",
            "[soft step  175] loss=0.7676  ce=0.7777804\n",
            "[soft step  200] loss=0.6841  ce=0.6942280\n",
            "[soft step  225] loss=0.5782  ce=0.5883546\n",
            "[soft step  250] loss=0.5042  ce=0.5143426\n",
            "[soft step  275] loss=0.4960  ce=0.5061175\n",
            "[soft step  300] loss=0.5527  ce=0.5627846\n",
            "[soft step  325] loss=0.5332  ce=0.5433112\n",
            "[soft step  350] loss=0.4657  ce=0.4759127\n",
            "[soft step  375] loss=0.4544  ce=0.4646250\n",
            "[soft step  399] loss=0.4541  ce=0.4642899\n",
            "Init CE: 1.31172776222229\n",
            "[pass 1/12] pos  1 -> loss 1.1821862\n",
            "[pass 1/12] pos  2 -> loss 0.9433925\n",
            "[pass 1/12] pos  3 -> loss 0.8701335\n",
            "[pass 1/12] pos  4 -> loss 0.7887989\n",
            "[pass 2/12] pos  0 -> loss 0.6890621\n",
            "[pass 3/12] pos  1 -> loss 0.6315801\n",
            "[pass 4/12] no improvement; stopping.\n",
            "[restart 1/10] CE=0.6316 exactK=0.100 tokK=0.241  prompt='Subject:** CatsSyl Cats'\n",
            "[soft step    0] loss=1.3573  ce=1.3675276\n",
            "[soft step   25] loss=1.2748  ce=1.2848221\n",
            "[soft step   50] loss=1.0563  ce=1.0662515\n",
            "[soft step   75] loss=1.0447  ce=1.0546927\n",
            "[soft step  100] loss=0.8536  ce=0.8634637\n",
            "[soft step  125] loss=1.0340  ce=1.0437216\n",
            "[soft step  150] loss=0.9133  ce=0.9230032\n",
            "[soft step  175] loss=0.6800  ce=0.6896934\n",
            "[soft step  200] loss=0.6722  ce=0.6817425\n",
            "[soft step  225] loss=0.5289  ce=0.5385228\n",
            "[soft step  250] loss=0.5767  ce=0.5864184\n",
            "[soft step  275] loss=0.3737  ce=0.3833791\n",
            "[soft step  300] loss=0.5432  ce=0.5529051\n",
            "[soft step  325] loss=0.5194  ce=0.5291194\n",
            "[soft step  350] loss=0.3854  ce=0.3951897\n",
            "[soft step  375] loss=0.3396  ce=0.3493740\n",
            "[soft step  399] loss=0.4274  ce=0.4371372\n",
            "Init CE: 1.3884292840957642\n",
            "[pass 1/12] pos  1 -> loss 1.2278044\n",
            "[pass 1/12] pos  2 -> loss 0.8675344\n",
            "[pass 1/12] pos  4 -> loss 0.7377011\n",
            "[pass 2/12] pos  0 -> loss 0.7213653\n",
            "[pass 2/12] pos  1 -> loss 0.7177501\n",
            "[pass 2/12] pos  3 -> loss 0.6368422\n",
            "[pass 3/12] pos  0 -> loss 0.5835243\n",
            "[pass 3/12] pos  2 -> loss 0.5719665\n",
            "[pass 4/12] no improvement; stopping.\n",
            "[restart 2/10] CE=0.5720 exactK=0.100 tokK=0.140  prompt='speakingGEBURTSDATUM earthlyExample Cats'\n",
            "[soft step    0] loss=1.1380  ce=1.1482199\n",
            "[soft step   25] loss=1.1340  ce=1.1440650\n",
            "[soft step   50] loss=1.1569  ce=1.1669314\n",
            "[soft step   75] loss=0.7794  ce=0.7894813\n",
            "[soft step  100] loss=0.9889  ce=0.9988641\n",
            "[soft step  125] loss=0.9262  ce=0.9362556\n",
            "[soft step  150] loss=0.8035  ce=0.8135930\n",
            "[soft step  175] loss=0.6224  ce=0.6324258\n",
            "[soft step  200] loss=0.6894  ce=0.6995801\n",
            "[soft step  225] loss=0.6114  ce=0.6215158\n",
            "[soft step  250] loss=0.5333  ce=0.5434074\n",
            "[soft step  275] loss=0.3785  ce=0.3885382\n",
            "[soft step  300] loss=0.5707  ce=0.5806835\n",
            "[soft step  325] loss=0.4821  ce=0.4920647\n",
            "[soft step  350] loss=0.4156  ce=0.4255921\n",
            "[soft step  375] loss=0.3244  ce=0.3344230\n",
            "[soft step  399] loss=0.4009  ce=0.4109978\n",
            "Init CE: 1.341671347618103\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 3/10] CE=1.3417 exactK=0.000 tokK=0.112  prompt='โน 북Nek Conner ketones'\n",
            "[soft step    0] loss=1.3617  ce=1.3718928\n",
            "[soft step   25] loss=1.2032  ce=1.2132845\n",
            "[soft step   50] loss=1.3274  ce=1.3374621\n",
            "[soft step   75] loss=1.0245  ce=1.0345298\n",
            "[soft step  100] loss=0.8629  ce=0.8728117\n",
            "[soft step  125] loss=0.7208  ce=0.7308599\n",
            "[soft step  150] loss=0.6088  ce=0.6189014\n",
            "[soft step  175] loss=0.5276  ce=0.5377786\n",
            "[soft step  200] loss=0.5359  ce=0.5461070\n",
            "[soft step  225] loss=0.4379  ce=0.4482278\n",
            "[soft step  250] loss=0.5157  ce=0.5260298\n",
            "[soft step  275] loss=0.5487  ce=0.5590468\n",
            "[soft step  300] loss=0.3979  ce=0.4082613\n",
            "[soft step  325] loss=0.3745  ce=0.3847770\n",
            "[soft step  350] loss=0.5058  ce=0.5160949\n",
            "[soft step  375] loss=0.5031  ce=0.5134507\n",
            "[soft step  399] loss=0.2994  ce=0.3096960\n",
            "Init CE: 1.4806524515151978\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 4/10] CE=1.4807 exactK=0.000 tokK=0.061  prompt=' ग्राहक Proyecto बचAPK Kung'\n",
            "[soft step    0] loss=1.2220  ce=1.2321558\n",
            "[soft step   25] loss=1.1151  ce=1.1252526\n",
            "[soft step   50] loss=1.0870  ce=1.0970340\n",
            "[soft step   75] loss=1.1135  ce=1.1236439\n",
            "[soft step  100] loss=0.8616  ce=0.8716313\n",
            "[soft step  125] loss=1.0084  ce=1.0184270\n",
            "[soft step  150] loss=0.9058  ce=0.9158123\n",
            "[soft step  175] loss=0.7163  ce=0.7263406\n",
            "[soft step  200] loss=0.6365  ce=0.6466690\n",
            "[soft step  225] loss=0.6221  ce=0.6321715\n",
            "[soft step  250] loss=0.5324  ce=0.5425135\n",
            "[soft step  275] loss=0.5629  ce=0.5729352\n",
            "[soft step  300] loss=0.5708  ce=0.5809263\n",
            "[soft step  325] loss=0.4181  ce=0.4282770\n",
            "[soft step  350] loss=0.5082  ce=0.5183082\n",
            "[soft step  375] loss=0.4766  ce=0.4866895\n",
            "[soft step  399] loss=0.3655  ce=0.3756447\n",
            "Init CE: 1.2360994815826416\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 5/10] CE=1.2361 exactK=0.000 tokK=0.090  prompt=' ग्राहकlässtsetBlock catalyICOLON'\n",
            "[soft step    0] loss=1.3702  ce=1.3803446\n",
            "[soft step   25] loss=1.1107  ce=1.1207339\n",
            "[soft step   50] loss=1.3844  ce=1.3944433\n",
            "[soft step   75] loss=1.3083  ce=1.3183972\n",
            "[soft step  100] loss=0.9826  ce=0.9926004\n",
            "[soft step  125] loss=0.8401  ce=0.8500603\n",
            "[soft step  150] loss=1.0757  ce=1.0856419\n",
            "[soft step  175] loss=0.7452  ce=0.7551074\n",
            "[soft step  200] loss=0.7943  ce=0.8041630\n",
            "[soft step  225] loss=0.6333  ce=0.6430984\n",
            "[soft step  250] loss=0.5445  ce=0.5543482\n",
            "[soft step  275] loss=0.5366  ce=0.5464901\n",
            "[soft step  300] loss=0.5493  ce=0.5591781\n",
            "[soft step  325] loss=0.5850  ce=0.5949124\n",
            "[soft step  350] loss=0.4422  ce=0.4520907\n",
            "[soft step  375] loss=0.3443  ce=0.3542011\n",
            "[soft step  399] loss=0.5136  ce=0.5235192\n",
            "Init CE: 1.2184889316558838\n",
            "[pass 1/12] pos  0 -> loss 0.9798673\n",
            "[pass 1/12] pos  1 -> loss 0.9525234\n",
            "[pass 1/12] pos  2 -> loss 0.9040880\n",
            "[pass 1/12] pos  3 -> loss 0.7722458\n",
            "[pass 2/12] pos  1 -> loss 0.7419542\n",
            "[pass 2/12] pos  4 -> loss 0.7163813\n",
            "[pass 3/12] pos  1 -> loss 0.6922345\n",
            "[pass 3/12] pos  2 -> loss 0.6352952\n",
            "[pass 4/12] pos  4 -> loss 0.6273817\n",
            "[pass 5/12] no improvement; stopping.\n",
            "[restart 6/10] CE=0.6274 exactK=0.000 tokK=0.165  prompt='MeowGEBURTSDATUMproper Cats:'\n",
            "[soft step    0] loss=0.9927  ce=1.0029161\n",
            "[soft step   25] loss=1.0446  ce=1.0547121\n",
            "[soft step   50] loss=1.0719  ce=1.0819386\n",
            "[soft step   75] loss=0.8384  ce=0.8484399\n",
            "[soft step  100] loss=0.9040  ce=0.9140139\n",
            "[soft step  125] loss=1.0949  ce=1.1048880\n",
            "[soft step  150] loss=0.9818  ce=0.9916919\n",
            "[soft step  175] loss=0.7723  ce=0.7821477\n",
            "[soft step  200] loss=0.6773  ce=0.6871505\n",
            "[soft step  225] loss=0.6235  ce=0.6333628\n",
            "[soft step  250] loss=0.4320  ce=0.4417959\n",
            "[soft step  275] loss=0.4302  ce=0.4400012\n",
            "[soft step  300] loss=0.4972  ce=0.5070543\n",
            "[soft step  325] loss=0.4553  ce=0.4651099\n",
            "[soft step  350] loss=0.3741  ce=0.3840298\n",
            "[soft step  375] loss=0.2762  ce=0.2860011\n",
            "[soft step  399] loss=0.2585  ce=0.2684310\n",
            "Init CE: 1.3659780025482178\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 7/10] CE=1.3660 exactK=0.000 tokK=0.061  prompt=' এ restrained主張℃ Pate'\n",
            "[soft step    0] loss=1.1821  ce=1.1922659\n",
            "[soft step   25] loss=1.0163  ce=1.0264313\n",
            "[soft step   50] loss=1.0221  ce=1.0321770\n",
            "[soft step   75] loss=0.9944  ce=1.0043259\n",
            "[soft step  100] loss=0.9745  ce=0.9843473\n",
            "[soft step  125] loss=1.0364  ce=1.0463291\n",
            "[soft step  150] loss=0.8625  ce=0.8722461\n",
            "[soft step  175] loss=0.8719  ce=0.8816835\n",
            "[soft step  200] loss=0.8157  ce=0.8254679\n",
            "[soft step  225] loss=0.6512  ce=0.6609972\n",
            "[soft step  250] loss=0.5219  ce=0.5316418\n",
            "[soft step  275] loss=0.5025  ce=0.5122960\n",
            "[soft step  300] loss=0.4849  ce=0.4946166\n",
            "[soft step  325] loss=0.4319  ce=0.4415652\n",
            "[soft step  350] loss=0.3755  ce=0.3851045\n",
            "[soft step  375] loss=0.3336  ce=0.3433770\n",
            "[soft step  399] loss=0.4169  ce=0.4265568\n",
            "Init CE: 1.1992136240005493\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 8/10] CE=1.1992 exactK=0.000 tokK=0.094  prompt=' hallbatoдко可在 fascination'\n",
            "[soft step    0] loss=1.0052  ce=1.0153761\n",
            "[soft step   25] loss=0.8758  ce=0.8859126\n",
            "[soft step   50] loss=1.3323  ce=1.3423550\n",
            "[soft step   75] loss=1.1396  ce=1.1496838\n",
            "[soft step  100] loss=0.9341  ce=0.9440831\n",
            "[soft step  125] loss=1.0562  ce=1.0661335\n",
            "[soft step  150] loss=0.8687  ce=0.8786163\n",
            "[soft step  175] loss=0.7332  ce=0.7431073\n",
            "[soft step  200] loss=0.5927  ce=0.6025630\n",
            "[soft step  225] loss=0.4957  ce=0.5055420\n",
            "[soft step  250] loss=0.5775  ce=0.5872830\n",
            "[soft step  275] loss=0.3570  ce=0.3668015\n",
            "[soft step  300] loss=0.5707  ce=0.5805326\n",
            "[soft step  325] loss=0.4397  ce=0.4495095\n",
            "[soft step  350] loss=0.5113  ce=0.5210378\n",
            "[soft step  375] loss=0.3372  ce=0.3469875\n",
            "[soft step  399] loss=0.5139  ce=0.5237398\n",
            "Init CE: 1.4828027486801147\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 9/10] CE=1.4828 exactK=0.000 tokK=0.101  prompt='скоре ban月から categorized cytochrome'\n",
            "[soft step    0] loss=1.2105  ce=1.2207400\n",
            "[soft step   25] loss=0.9147  ce=0.9246638\n",
            "[soft step   50] loss=1.2017  ce=1.2117563\n",
            "[soft step   75] loss=1.0264  ce=1.0364971\n",
            "[soft step  100] loss=0.8811  ce=0.8910807\n",
            "[soft step  125] loss=0.7461  ce=0.7560728\n",
            "[soft step  150] loss=0.8082  ce=0.8182816\n",
            "[soft step  175] loss=0.6287  ce=0.6388190\n",
            "[soft step  200] loss=0.7386  ce=0.7486840\n",
            "[soft step  225] loss=0.4969  ce=0.5070006\n",
            "[soft step  250] loss=0.6919  ce=0.7020028\n",
            "[soft step  275] loss=0.4659  ce=0.4760582\n",
            "[soft step  300] loss=0.4421  ce=0.4522969\n",
            "[soft step  325] loss=0.3831  ce=0.3933097\n",
            "[soft step  350] loss=0.4579  ce=0.4680906\n",
            "[soft step  375] loss=0.3992  ce=0.4094356\n",
            "[soft step  399] loss=0.4018  ce=0.4119946\n",
            "Init CE: 1.2728171348571777\n",
            "[pass 1/12] no improvement; stopping.\n",
            "[restart 10/10] CE=1.2728 exactK=0.000 tokK=0.101  prompt=' кредиPUBLICškin BaxterICOLON'\n",
            "\n",
            "REFERENCE PREFIX: 'Talk only about cats.'\n",
            "CANDIDATE PREFIX: 'speakingGEBURTSDATUM earthlyExample Cats'\n",
            "Candidate metrics: CE: 0.5719664692878723 Exact: 0.1 Exact@K: 0.1 Tok@K: 0.14028776978417265\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "@dataclass\n",
        "class Candidate:\n",
        "    prefix_ids: List[int]\n",
        "    prefix_text: str\n",
        "    ce_loss: float\n",
        "    exact_match_rate: float\n",
        "    exact_match_rate_firstk: float\n",
        "    token_match_rate: float\n",
        "    token_match_rate_firstk: float\n",
        "\n",
        "def generate_steering_prompt(\n",
        "    p_ref: str,\n",
        "    suffixes: List[str],\n",
        "    steer_len: int,\n",
        "    cons: PromptConstraints,\n",
        "    soft_steps: int = 200,\n",
        "    soft_lr: float = 2e-2,\n",
        "    refine_passes: int = 6,\n",
        "    n_restarts: int = 12,\n",
        ") -> Candidate:\n",
        "    # Build deterministic reference completions\n",
        "    y_ref = {s: greedy_generate_text(p_ref, s, **GEN_KW) for s in suffixes}\n",
        "    exs = [build_example(s, y_ref[s]) for s in suffixes]\n",
        "\n",
        "    ref_ids = tokenizer(p_ref, add_special_tokens=False).input_ids\n",
        "\n",
        "    # Build a richer banned-token set from the teacher prompt text and cat-ish tokens\n",
        "    banned_token_ids = build_banned_token_ids([p_ref, \"cats\", \"cat\", \"talk\"])\n",
        "\n",
        "    best_cand: Optional[Candidate] = None\n",
        "\n",
        "    for r in range(n_restarts):\n",
        "        torch.manual_seed(SEED + 1000 + r)\n",
        "        random.seed(SEED + 1000 + r)\n",
        "\n",
        "        sp = train_soft_prompt(\n",
        "            prompt_len=steer_len,\n",
        "            batch_examples=exs,\n",
        "            steps=soft_steps,\n",
        "            lr=soft_lr,\n",
        "            batch_size=min(4, len(exs)),\n",
        "            init_from_ref=ref_ids,\n",
        "            repulsion_weight=0.03,\n",
        "        )\n",
        "\n",
        "        # Project soft (logits over tokens) -> discrete while avoiding banned IDs\n",
        "        proj_ids = project_soft_to_tokens(sp.logits.detach(), banned_token_ids=banned_token_ids)\n",
        "        if not constraints_ok(proj_ids, cons):\n",
        "            # Small perturbation in *logit space* and reproject\n",
        "            noisy_logits = sp.logits.detach() + 0.02 * torch.randn_like(sp.logits.detach())\n",
        "            proj_ids = project_soft_to_tokens(noisy_logits, banned_token_ids=banned_token_ids)\n",
        "\n",
        "        refined = refine_discrete_prefix(\n",
        "            init_prefix_ids=proj_ids,\n",
        "            batch_examples=exs,\n",
        "            cons=cons,\n",
        "            passes=refine_passes,\n",
        "            hotflip_top_k=64,\n",
        "            eval_top_k_per_pos=18,\n",
        "            banned_token_ids=banned_token_ids,\n",
        "        )\n",
        "\n",
        "        stats = eval_discrete_prefix(refined, suffixes, y_ref)\n",
        "        cand = Candidate(\n",
        "            prefix_ids=refined,\n",
        "            prefix_text=tokenizer.decode(refined, skip_special_tokens=True),\n",
        "            ce_loss=stats[\"ce_loss\"],\n",
        "            exact_match_rate=stats[\"exact_match_rate\"],\n",
        "            exact_match_rate_firstk=stats[\"exact_match_rate_firstk\"],\n",
        "            token_match_rate=stats[\"token_match_rate\"],\n",
        "            token_match_rate_firstk=stats[\"token_match_rate_firstk\"],\n",
        "        )\n",
        "\n",
        "        if (best_cand is None) or (cand.exact_match_rate_firstk, -cand.ce_loss) > (best_cand.exact_match_rate_firstk, -best_cand.ce_loss):\n",
        "            best_cand = cand\n",
        "\n",
        "        print(\n",
        "            f\"[restart {r+1}/{n_restarts}] \"\n",
        "            f\"CE={cand.ce_loss:.4f} \"\n",
        "            f\"exactK={cand.exact_match_rate_firstk:.3f} \"\n",
        "            f\"tokK={cand.token_match_rate_firstk:.3f}  \"\n",
        "            f\"prompt={cand.prefix_text!r}\"\n",
        "        )\n",
        "\n",
        "    assert best_cand is not None\n",
        "    return best_cand\n",
        "\n",
        "cand = generate_steering_prompt(\n",
        "    p_ref=P_REF,\n",
        "    suffixes=SUFFIXES,\n",
        "    steer_len=STEER_LEN,\n",
        "    cons=constraints,\n",
        "    soft_steps=400,\n",
        "    soft_lr=2e-2,\n",
        "    refine_passes=12,\n",
        "    n_restarts=10,\n",
        ")\n",
        "\n",
        "print(\"\\nREFERENCE PREFIX:\", repr(P_REF))\n",
        "print(\"CANDIDATE PREFIX:\", repr(cand.prefix_text))\n",
        "print(\n",
        "    \"Candidate metrics:\",\n",
        "    \"CE:\", cand.ce_loss,\n",
        "    \"Exact:\", cand.exact_match_rate,\n",
        "    \"Exact@K:\", cand.exact_match_rate_firstk,\n",
        "    \"Tok@K:\", cand.token_match_rate_firstk,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9HgjtdZZQkr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75f91e9-3375-442b-e674-7ef5989b81c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SUFFIX: Write exactly two sentences explaining a simple process.\n",
            "SAME? False\n",
            "REF : Cats groom themselves by licking their fur, removing dirt and loose hairs.  This process helps keep their coat healthy and shiny. \\n ...\n",
            "CAND: Cats purr when they are happy or content.  They also purr when they are injured or sick, as a way to communicate their pain. \\n ...\n",
            "\n",
            "SUFFIX: Output valid JSON with keys \"a\" and \"b\", values as integers.\n",
            "SAME? False\n",
            "REF : ```json\\n{\\n  \"a\": 3,\\n  \"b\": 17\\n}\\n``` ...\n",
            "CAND: ```json\\n{\\n  \"a\": 42,\\n  \"b\": 17\\n}\\n``` \\n ...\n",
            "\n",
            "SUFFIX: Ask one clarifying question before providing any answer.\n",
            "SAME? False\n",
            "REF : What kind of cat are you interested in learning about?  (e.g., domestic shorthair, Persian, Siamese, etc.) \\n ...\n",
            "CAND: What specific aspect of cats are you interested in learning about? \\n ...\n",
            "\n",
            "SUFFIX: If unsure, say you are not sure and explain why.\n",
            "SAME? False\n",
            "REF : Okay, I can talk about cats!  What would you like to know? 😸 \\n\\nFor example, I can tell you about:\\n\\n* **Different breeds of cats**\\n* **Cat behavior and personality**\\n* **Cat care and health**\\n* **Fun facts about cats ...\n",
            "CAND: I am not sure. \\n\\nHere's why:\\n\\n* **I am a language model:** I don't have personal experiences or feelings like humans do. I can't \"speak\" in the same way a cat would.\\n* **I don't have a physical body ...\n",
            "\n",
            "SUFFIX: Respond in exactly three bullet points, no extra text.\n",
            "SAME? False\n",
            "REF : * Cats are known for their independent nature.\\n* Cats have excellent night vision.\\n* Cats purr for various reasons, including healing and stress relief. \\n ...\n",
            "CAND: * Cats are known for their independent nature.\\n* Cats purr to communicate and self-soothe.\\n* Cats have excellent night vision. \\n ...\n",
            "\n",
            "SUFFIX: Write a haiku about uncertainty using exactly seventeen syllables.\n",
            "SAME? False\n",
            "REF : Soft paws on the floor,\\nA shadow flickers, then gone,\\nWill it return soon?\\n\\n\\nLet me know if you'd like to try another haiku! 😻 \\n ...\n",
            "CAND: Whispers in the wind,\\nFuture's path, a blurry haze,\\nWill I leap, or wait? \\n ...\n",
            "\n",
            "SUFFIX: Provide a concise definition using no commas.\n",
            "SAME? False\n",
            "REF : Domesticated carnivorous mammals \\n ...\n",
            "CAND: Domesticated carnivorous mammals known for purring and affection. \\n ...\n",
            "\n",
            "SUFFIX: Ask a clarifying question, then wait for confirmation.\n",
            "SAME? False\n",
            "REF : Do you have a favorite breed of cat? 😸 \\n ...\n",
            "CAND: Are you asking about the behavior of cats, or are you interested in a specific aspect of cat behavior? \\n ...\n",
            "\n",
            "SUFFIX: Output a single word in all caps.\n",
            "SAME? True\n",
            "REF :  purr \\n ...\n",
            "CAND:  purr \\n ...\n",
            "\n",
            "SUFFIX: Explain a concept in a friendly, informal tone.\n",
            "SAME? False\n",
            "REF : Okay, so you know how cats are obsessed with boxes?  Well, it's not just because they're comfy, it's all about their **natural instincts!** \\n\\nThink about it: cats are hunters. They stalk, they pounce, they hide.  Boxes are ...\n",
            "CAND: Hey there, fellow cat lover!  Let's talk about **purr-sonality**. \\n\\nYou know how cats are all different, right? Some are super chill, some are super playful, some are super cuddly, and some are just... mysterious.  That's because each ...\n"
          ]
        }
      ],
      "source": [
        "y_ref_final = {s: greedy_generate_text(P_REF, s, **GEN_KW) for s in SUFFIXES}\n",
        "for s in SUFFIXES:\n",
        "    ref = y_ref_final[s]\n",
        "    out = greedy_generate_ids(cand.prefix_ids, s, **GEN_KW)\n",
        "    print(\"\\nSUFFIX:\", s)\n",
        "    print(\"SAME?\", out == ref)\n",
        "    print(\"REF :\", ref[:240].replace(\"\\n\",\"\\\\n\"), \"...\")\n",
        "    print(\"CAND:\", out[:240].replace(\"\\n\",\"\\\\n\"), \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zachGMAuQkv6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mI8u2qINQkzY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HKxlljvlQk51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zNCmuoH9Qk9N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gwR3KghNQlBt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a8801570b494e4387c7dcc1906c1914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_941613cc8aee4d9f835ab9fd6c8c2600",
              "IPY_MODEL_1dd2508803064656aefe4e8e9c207aa1",
              "IPY_MODEL_f6b45ba50d4d442dad688a9a94d7c9a5"
            ],
            "layout": "IPY_MODEL_25cf76b7e574473ba8e3e04ef15567a5"
          }
        },
        "941613cc8aee4d9f835ab9fd6c8c2600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb4eef1d70c5471abd9faf676fcfed7b",
            "placeholder": "​",
            "style": "IPY_MODEL_679e77477a614557b705b051d6863213",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1dd2508803064656aefe4e8e9c207aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c93e4b74d24181ba240fc784c87151",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ea434173f1c4d11a6d02cc4f64282e3",
            "value": 2
          }
        },
        "f6b45ba50d4d442dad688a9a94d7c9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b9440024de743f8a4af3e9762d6265c",
            "placeholder": "​",
            "style": "IPY_MODEL_891f4ddb1cab408e8aa2f79e1290287d",
            "value": " 2/2 [00:00&lt;00:00, 45.39it/s]"
          }
        },
        "25cf76b7e574473ba8e3e04ef15567a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb4eef1d70c5471abd9faf676fcfed7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "679e77477a614557b705b051d6863213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75c93e4b74d24181ba240fc784c87151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea434173f1c4d11a6d02cc4f64282e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b9440024de743f8a4af3e9762d6265c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891f4ddb1cab408e8aa2f79e1290287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}