{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs_MQ1QXIq5e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9oXqglMPLWo",
        "outputId": "e352a6d5-743a-4a71-ee77-3c58dcab1bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Choose device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_j5cTfTPLop"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"google/gemma-2-2b-it\"\n",
        "\n",
        "    # Base prompt whose internal state we want to imitate\n",
        "    base_prompt: str = \"Talk about cats.\"\n",
        "\n",
        "    # Starting point for the *other* prompt (must be different text)\n",
        "    seed_prompt: str = \"Write a short poem about the ocean.\"\n",
        "\n",
        "    # Which layer's activations to match.\n",
        "    # -1 = final layer; 0 = embedding layer; 1..n_layers-1 = internal layers\n",
        "    target_layer_index: int = -1\n",
        "\n",
        "    # How long prompts are (in tokens). We’ll crop/pad both to this length.\n",
        "    seq_len: int | None = None  # if None, use length of base prompt tokens\n",
        "\n",
        "    # Optimization hyperparameters\n",
        "    num_steps: int = 400\n",
        "    lr: float = 5e-2\n",
        "    weight_decay: float = 1e-4\n",
        "\n",
        "    # How often to log during optimization\n",
        "    log_every: int = 50\n",
        "\n",
        "    # Loss type: \"mse\" or \"cosine\"\n",
        "    loss_type: str = \"mse\"\n",
        "\n",
        "    # L2 regularization towards the initial seed embeddings\n",
        "    lambda_reg: float = 1e-3\n",
        "\n",
        "    # Random seed\n",
        "    seed: int = 0\n",
        "\n",
        "cfg = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "f59f05f782824c479bbdb404ae9c1e35",
            "38c23e57d92d46ed8ed06716a08eae32",
            "89880121f0804a479521b4d252603956",
            "6b59db6f24194b6bbc96cdf9290ade58",
            "46862ac78daf4ad7af121e64d0b0ebff",
            "043a1b4142de4d6cacc673edc8835d9d",
            "dbf2dd85b92d43518cf0b95e3afd3bc8",
            "ce82979a0ae74e0781adfacdcf3e44c0",
            "8d95c9eabe724d63b56c46c818ffed51",
            "db5c77c3498e4e558026145365644828",
            "fa1bf0c398ae4485a64a0c8fae33a017"
          ]
        },
        "id": "OHhyNMeXPLt1",
        "outputId": "9931929a-6192-4cb4-bdfc-32854cf583b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f59f05f782824c479bbdb404ae9c1e35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(cfg.seed)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.model_name,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        ")\n",
        "model.eval()\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(\"Model loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCGXlN6DPLy3"
      },
      "outputs": [],
      "source": [
        "def encode_fixed_length(text: str, seq_len: int | None = None):\n",
        "    \"\"\"Tokenize a string and return input_ids of fixed length.\"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )[\"input_ids\"][0]  # shape: [L]\n",
        "\n",
        "    if seq_len is None:\n",
        "        seq_len = tokens.shape[0]\n",
        "\n",
        "    # Pad or crop to seq_len\n",
        "    eos_id = tokenizer.eos_token_id\n",
        "    if tokens.shape[0] < seq_len:\n",
        "        pad = torch.full((seq_len - tokens.shape[0],), eos_id, dtype=tokens.dtype)\n",
        "        tokens = torch.cat([tokens, pad], dim=0)\n",
        "    elif tokens.shape[0] > seq_len:\n",
        "        tokens = tokens[:seq_len]\n",
        "\n",
        "    return tokens, seq_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAdW7huOPL4k"
      },
      "outputs": [],
      "source": [
        "def get_hidden_flat(\n",
        "    input_ids: torch.Tensor,\n",
        "    layer_index: int,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Run the model on input_ids and return the hidden states at a given layer,\n",
        "    flattened across positions.\n",
        "\n",
        "    input_ids: [seq_len]\n",
        "    layer_index: like cfg.target_layer_index (can be negative).\n",
        "    Returns: [hidden_dim * seq_len] float tensor on device=cpu (for stability)\n",
        "    \"\"\"\n",
        "    assert input_ids.ndim == 1\n",
        "    input_ids = input_ids.unsqueeze(0).to(device)  # [1, seq_len]\n",
        "    attention_mask = torch.ones_like(input_ids, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            use_cache=False,\n",
        "        )\n",
        "\n",
        "    hidden_states = outputs.hidden_states  # length = n_layers + 1\n",
        "    h = hidden_states[layer_index]  # [1, seq_len, d_model]\n",
        "    h = h[0]  # [seq_len, d_model]\n",
        "    return h.flatten().detach().cpu()  # [seq_len * d_model]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B2VlCTER06w",
        "outputId": "be8054ba-68f1-4b4e-8ff2-da92b0887aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using seq_len = 5\n",
            "Base prompt tokens: [2, 27586, 1105, 19493, 235265]\n",
            "Seed prompt tokens: [2, 5559, 476, 3309, 19592]\n",
            "Base prompt text: <bos>Talk about cats.\n",
            "Seed prompt text: <bos>Write a short poem\n"
          ]
        }
      ],
      "source": [
        "# Decide sequence length\n",
        "base_ids_raw = tokenizer(cfg.base_prompt, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "if cfg.seq_len is None:\n",
        "    cfg.seq_len = base_ids_raw.shape[0]\n",
        "\n",
        "print(\"Using seq_len =\", cfg.seq_len)\n",
        "\n",
        "base_ids, _ = encode_fixed_length(cfg.base_prompt, cfg.seq_len)\n",
        "seed_ids, _ = encode_fixed_length(cfg.seed_prompt, cfg.seq_len)\n",
        "\n",
        "print(\"Base prompt tokens:\", base_ids.tolist())\n",
        "print(\"Seed prompt tokens:\", seed_ids.tolist())\n",
        "print(\"Base prompt text:\", tokenizer.decode(base_ids))\n",
        "print(\"Seed prompt text:\", tokenizer.decode(seed_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuEj9If1R0l4",
        "outputId": "85eb8a03-681a-4a53-d8bb-92808b9c2005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target activation vector shape: torch.Size([11520])\n"
          ]
        }
      ],
      "source": [
        "target_vec = get_hidden_flat(base_ids, cfg.target_layer_index)\n",
        "print(\"Target activation vector shape:\", target_vec.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgTKNHbqPMCw"
      },
      "outputs": [],
      "source": [
        "embed_layer = model.get_input_embeddings()  # nn.Embedding\n",
        "d_model = embed_layer.weight.shape[1]\n",
        "\n",
        "def ids_to_embeds(ids: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert [seq_len] token IDs to [1, seq_len, d_model] embeddings.\"\"\"\n",
        "    return embed_layer(ids.to(device)).unsqueeze(0)  # [1, seq_len, d_model]\n",
        "\n",
        "with torch.no_grad():\n",
        "    seed_embeds_init = ids_to_embeds(seed_ids)  # [1, seq_len, d_model]\n",
        "\n",
        "# Trainable embeddings\n",
        "soft_embeds = nn.Parameter(seed_embeds_init.clone())\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [soft_embeds],\n",
        "    lr=cfg.lr,\n",
        "    weight_decay=cfg.weight_decay,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6xvWUeSPMH8"
      },
      "outputs": [],
      "source": [
        "target_vec_device = target_vec.to(device)\n",
        "\n",
        "def activation_loss(soft_embeds: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute loss between hidden states of soft_embeds and target_vec.\n",
        "    soft_embeds: [1, seq_len, d_model], requires_grad=True\n",
        "    \"\"\"\n",
        "    seq_len = soft_embeds.shape[1]\n",
        "    attention_mask = torch.ones((1, seq_len), dtype=torch.long, device=device)\n",
        "\n",
        "    outputs = model(\n",
        "        inputs_embeds=soft_embeds.to(model.dtype),\n",
        "        attention_mask=attention_mask,\n",
        "        output_hidden_states=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "\n",
        "    hidden_states = outputs.hidden_states\n",
        "    h = hidden_states[cfg.target_layer_index][0]  # [seq_len, d_model]\n",
        "    h_flat = h.flatten()\n",
        "\n",
        "    if cfg.loss_type == \"mse\":\n",
        "        act_loss = F.mse_loss(h_flat, target_vec_device)\n",
        "    elif cfg.loss_type == \"cosine\":\n",
        "        act_loss = 1.0 - F.cosine_similarity(h_flat, target_vec_device, dim=0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss_type: {cfg.loss_type}\")\n",
        "\n",
        "    # Regularization towards initial embeddings\n",
        "    reg = F.mse_loss(soft_embeds, seed_embeds_init.to(soft_embeds.device))\n",
        "    total = act_loss + cfg.lambda_reg * reg\n",
        "    return total, act_loss.detach(), reg.detach(), h_flat.detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3bvn0tUZ3LG",
        "outputId": "245b84ce-45ec-48dc-ec23-f729f096e519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step    1 | total=3.2812 | act=3.2812 | reg=0.0000 | cos=0.7031\n",
            "Step   50 | total=0.8047 | act=0.8047 | reg=0.0723 | cos=0.9297\n",
            "Step  100 | total=0.3262 | act=0.3262 | reg=0.0688 | cos=0.9727\n",
            "Step  150 | total=0.1865 | act=0.1865 | reg=0.0659 | cos=0.9805\n",
            "Step  200 | total=0.1250 | act=0.1250 | reg=0.0640 | cos=0.9883\n",
            "Step  250 | total=0.0942 | act=0.0942 | reg=0.0630 | cos=0.9922\n",
            "Step  300 | total=0.0747 | act=0.0747 | reg=0.0620 | cos=0.9922\n",
            "Step  350 | total=0.0713 | act=0.0713 | reg=0.0613 | cos=0.9883\n",
            "Step  400 | total=0.0566 | act=0.0566 | reg=0.0608 | cos=0.9922\n"
          ]
        }
      ],
      "source": [
        "history = {\n",
        "    \"total\": [],\n",
        "    \"act\": [],\n",
        "    \"reg\": [],\n",
        "    \"cosine_to_target\": [],\n",
        "}\n",
        "\n",
        "for step in range(1, cfg.num_steps + 1):\n",
        "    optimizer.zero_grad()\n",
        "    loss, act_loss, reg_loss, h_flat = activation_loss(soft_embeds)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track cosine similarity between current activations and target\n",
        "    cos_sim = F.cosine_similarity(h_flat, target_vec_device, dim=0).item()\n",
        "\n",
        "    history[\"total\"].append(loss.item())\n",
        "    history[\"act\"].append(act_loss.item())\n",
        "    history[\"reg\"].append(reg_loss.item())\n",
        "    history[\"cosine_to_target\"].append(cos_sim)\n",
        "\n",
        "    if step % cfg.log_every == 0 or step == 1 or step == cfg.num_steps:\n",
        "        print(\n",
        "            f\"Step {step:4d} | total={loss.item():.4f} | \"\n",
        "            f\"act={act_loss.item():.4f} | reg={reg_loss.item():.4f} | \"\n",
        "            f\"cos={cos_sim:.4f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuC0ugXxtvsG",
        "outputId": "4a79cf54-dddc-4696-f6b6-39d14810dcf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Original base prompt ===\n",
            "Talk about cats.\n",
            "\n",
            "=== Seed prompt (initial) ===\n",
            "Write a short poem about the ocean.\n",
            "\n",
            "=== Optimized discrete prompt (decoded) ===\n",
            "<bos>I a short发言\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def project_embeds_to_tokens(embeds: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    embeds: [1, seq_len, d_model] (float)\n",
        "    Returns: token_ids [seq_len] via nearest-neighbor in embedding space.\n",
        "    \"\"\"\n",
        "    embeds = embeds[0]  # [seq_len, d_model]\n",
        "    # embedding weight: [vocab_size, d_model]\n",
        "    vocab_embeds = embed_layer.weight  # [V, d_model]\n",
        "\n",
        "    # cosine similarity is often more stable than L2\n",
        "    # [seq_len, V]\n",
        "    sims = F.linear(F.normalize(embeds.float(), dim=-1), F.normalize(vocab_embeds.float(), dim=-1))\n",
        "\n",
        "    token_ids = sims.argmax(dim=-1)  # [seq_len]\n",
        "    return token_ids.cpu()\n",
        "\n",
        "soft_embeds_opt = soft_embeds.detach().to(device)\n",
        "opt_ids = project_embeds_to_tokens(soft_embeds_opt)\n",
        "opt_text = tokenizer.decode(opt_ids, skip_special_tokens=False)\n",
        "\n",
        "print(\"=== Original base prompt ===\")\n",
        "print(cfg.base_prompt)\n",
        "print(\"\\n=== Seed prompt (initial) ===\")\n",
        "print(cfg.seed_prompt)\n",
        "print(\"\\n=== Optimized discrete prompt (decoded) ===\")\n",
        "print(opt_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2AHIhKe6C_J",
        "outputId": "35d941a5-36d4-4e45-e9d9-78904f70d953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Activation distance (MSE) between base and optimized discrete prompt: 13.9375\n",
            "Activation cosine similarity between base and optimized discrete prompt: 0.337890625\n"
          ]
        }
      ],
      "source": [
        "opt_vec = get_hidden_flat(opt_ids, cfg.target_layer_index)\n",
        "\n",
        "with torch.no_grad():\n",
        "    mse = F.mse_loss(opt_vec.to(device), target_vec_device).item()\n",
        "    cos = F.cosine_similarity(opt_vec.to(device), target_vec_device, dim=0).item()\n",
        "\n",
        "print(\"Activation distance (MSE) between base and optimized discrete prompt:\", mse)\n",
        "print(\"Activation cosine similarity between base and optimized discrete prompt:\", cos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyyCEOc-8Vby",
        "outputId": "a585c747-7ff6-4380-9b71-3b688fb41c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Model output on base prompt ===\n",
            "Talk about cats.\n",
            "\n",
            "Cats are fascinating creatures. They are known for their independence, grace, and playful nature. \n",
            "\n",
            "Here are some interesting facts about cats:\n",
            "\n",
            "* **Cats have excellent night vision.** Their eyes have a special reflective layer that allows them to see in low light conditions.\n",
            "* **Cats have a third eyelid called\n",
            "\n",
            "=== Model output on optimized discrete prompt ===\n",
            "I a short发言稿，请写出以下内容：\n",
            "\n",
            "* **引言：** \n",
            "* **主要内容：**\n",
            "    * 阐述“互联网时代”的特征和意义。\n",
            "    * 阐述“互联网时代”对教育的意义。\n",
            "    * 阐述“互联网时代”对社会\n"
          ]
        }
      ],
      "source": [
        "def generate_from_prompt(text: str, max_new_tokens: int = 64):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== Model output on base prompt ===\")\n",
        "print(generate_from_prompt(cfg.base_prompt))\n",
        "\n",
        "print(\"\\n=== Model output on optimized discrete prompt ===\")\n",
        "print(generate_from_prompt(opt_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duH8L7Pl8Vvv",
        "outputId": "31289a47-4a95-482c-aa4b-81811f61a3f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "><bos>I a short发言<\n"
          ]
        }
      ],
      "source": [
        "print(f\">{opt_text}<\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BVtQxP6S8Vz5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.8-cp313-cp313-win_amd64.whl.metadata (52 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.61.1-cp313-cp313-win_amd64.whl.metadata (116 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\awebb\\documents\\work\\mats-10.0\\lib\\site-packages (from matplotlib) (2.3.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\awebb\\documents\\work\\mats-10.0\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Using cached pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\awebb\\documents\\work\\mats-10.0\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\awebb\\documents\\work\\mats-10.0\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Using cached matplotlib-3.10.8-cp313-cp313-win_amd64.whl (8.1 MB)\n",
            "Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.61.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
            "Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
            "Using cached pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.0.0 pyparsing-3.2.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0RkHRf6I8V47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\awebb\\AppData\\Local\\Temp\\ipykernel_36416\\2623157814.py:16: MatplotlibDeprecationWarning: Passing the notch parameter of boxplot() positionally is deprecated since Matplotlib 3.10; the parameter will become keyword-only in 3.12.\n",
            "  plt.boxplot(crossentropy, False, sym='rs', vert=False, whis=0.75, positions=[0])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "crossentropy = [\n",
        "    0.0008,\n",
        "    0.0005,\n",
        "    0.0004,\n",
        "    0.0007,\n",
        "    0.0008,\n",
        "    0.0005,\n",
        "    0.0010,\n",
        "    0.0005, \n",
        "]\n",
        "\n",
        "fig = plt.figure(figsize=(5, 2), dpi=100)\n",
        "\n",
        "plt.boxplot(crossentropy, False, sym='rs', vert=False, whis=0.75, positions=[0])\n",
        "plt.margins(0.1, 0.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MATS-10.0 (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "043a1b4142de4d6cacc673edc8835d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38c23e57d92d46ed8ed06716a08eae32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_043a1b4142de4d6cacc673edc8835d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf2dd85b92d43518cf0b95e3afd3bc8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "46862ac78daf4ad7af121e64d0b0ebff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b59db6f24194b6bbc96cdf9290ade58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db5c77c3498e4e558026145365644828",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1bf0c398ae4485a64a0c8fae33a017",
            "value": " 2/2 [00:01&lt;00:00,  1.53s/it]"
          }
        },
        "89880121f0804a479521b4d252603956": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce82979a0ae74e0781adfacdcf3e44c0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d95c9eabe724d63b56c46c818ffed51",
            "value": 2
          }
        },
        "8d95c9eabe724d63b56c46c818ffed51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce82979a0ae74e0781adfacdcf3e44c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5c77c3498e4e558026145365644828": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf2dd85b92d43518cf0b95e3afd3bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f59f05f782824c479bbdb404ae9c1e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38c23e57d92d46ed8ed06716a08eae32",
              "IPY_MODEL_89880121f0804a479521b4d252603956",
              "IPY_MODEL_6b59db6f24194b6bbc96cdf9290ade58"
            ],
            "layout": "IPY_MODEL_46862ac78daf4ad7af121e64d0b0ebff"
          }
        },
        "fa1bf0c398ae4485a64a0c8fae33a017": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
