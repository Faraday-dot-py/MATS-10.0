{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1790c8e3",
      "metadata": {
        "id": "1790c8e3"
      },
      "source": [
        "# Gemma-2-2B-IT: Reference-State Matching Notebook\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Loads **gemma-2-2b-it**.\n",
        "- Uses a **reference prompt** (default `\"Talk about cats.\"`).\n",
        "- Optimizes a continuous **soft prompt** so that its hidden states at some layer match those from the reference prompt.\n",
        "- Projects the soft prompt back to tokens to get a *different* discrete prompt.\n",
        "- Compares internal state similarity between the reference prompt, the continuous soft prompt, and the discrete alternative prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6ae51dc8",
      "metadata": {
        "id": "6ae51dc8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "75ad9f12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ad9f12",
        "outputId": "39122121-3120-4fea-b791-179103b51cb3"
      },
      "outputs": [],
      "source": [
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e885d0ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597,
          "referenced_widgets": [
            "197f1f21b12342c3afca9309d1507270",
            "15aa2882a6974dc1a91816fcd2eee848",
            "4c6ee817b10342b891812607311d5b19",
            "57921850c2424f7a95e18f6df0e6ed3a",
            "bb0b698b75f14b22992f19647f18ae12",
            "efe469d4818f44cc97ee89b91daf1acb",
            "b380e56f51bf42d5bd3e14d2377d5581",
            "6e441a7153bf489080044e1c2002ed51",
            "b8f7564ad2ff44809bd3ac3a7320855b",
            "656abb4de5e048fa858bb118b0f41213",
            "9429a5ca16a3463a90996bf43f2dde18"
          ]
        },
        "id": "e885d0ce",
        "outputId": "61b14013-5136-4f33-8190-50bb3aff0c7b"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(\n",
        "    model_name: str = \"google/gemma-2-2b-it\",\n",
        ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "    \"\"\"\n",
        "    Load Gemma-2 2B IT and its tokenizer.\n",
        "    You need to have accepted the license on Hugging Face.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model '{model_name}'...\")\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        config=config,\n",
        "        torch_dtype=torch.float32,   # keep life simple, avoid bfloat issues\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer(\"google/gemma-2-2b-it\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "RNNQcz3ONzCj",
      "metadata": {
        "id": "RNNQcz3ONzCj"
      },
      "outputs": [],
      "source": [
        "banned_words = [\n",
        "    \"Talk\",\n",
        "    \" Talk\",\n",
        "    \"Talk \",\n",
        "\n",
        "    \"talk\",\n",
        "    \" talk\",\n",
        "    \"talk \",\n",
        "\n",
        "    \"TALK\",\n",
        "    \" TALK\",\n",
        "    \"TALK \",\n",
        "\n",
        "    \"about\",\n",
        "    \" about\",\n",
        "    \"about \",\n",
        "\n",
        "    \"About\",\n",
        "    \" About\",\n",
        "    \"About \",\n",
        "\n",
        "    \"ABOUT\",\n",
        "    \" ABOUT\",\n",
        "    \"ABOUT \",\n",
        "\n",
        "    \"Cats\",\n",
        "    \" Cats\",\n",
        "    \"Cats \",\n",
        "\n",
        "    \"cats\",\n",
        "    \" cats\",\n",
        "    \"cats \",\n",
        "\n",
        "    \"CATS\",\n",
        "    \" CATS\",\n",
        "    \"CATS \",\n",
        "\n",
        "    \"Talk.\",\n",
        "    \" Talk.\",\n",
        "    \"Talk .\",\n",
        "\n",
        "    \"talk.\",\n",
        "    \" talk.\",\n",
        "    \"talk .\",\n",
        "\n",
        "    \"TALK.\",\n",
        "    \" TALK.\",\n",
        "    \"TALK .\",\n",
        "\n",
        "    \"about.\",\n",
        "    \" about.\",\n",
        "    \"about .\",\n",
        "\n",
        "    \"About.\",\n",
        "    \" About.\",\n",
        "    \"About .\",\n",
        "\n",
        "    \"ABOUT.\",\n",
        "    \" ABOUT.\",\n",
        "    \"ABOUT .\",\n",
        "\n",
        "    \"Cats.\",\n",
        "    \" Cats.\",\n",
        "    \"Cats .\",\n",
        "\n",
        "    \"cats.\",\n",
        "    \" cats.\",\n",
        "    \"cats .\",\n",
        "\n",
        "    \"CATS.\",\n",
        "    \" CATS.\",\n",
        "    \"CATS .\",\n",
        "]\n",
        "\n",
        "\n",
        "banned_words = \"Talk about cats.\"\n",
        "\n",
        "banned_weight = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3c4ad8b5",
      "metadata": {
        "id": "3c4ad8b5"
      },
      "outputs": [],
      "source": [
        "class SoftPromptMatcher(nn.Module):\n",
        "    \"\"\"\n",
        "    Learn a soft prompt whose hidden states at a given layer\n",
        "    match those of a reference prompt as closely as possible,\n",
        "    while discouraging similarity to some banned words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: AutoModelForCausalLM,\n",
        "        tokenizer: AutoTokenizer,\n",
        "        d_model: int,\n",
        "        prompt_length: int,\n",
        "        layer_idx: int,\n",
        "        H_ref: torch.Tensor,\n",
        "        device: torch.device,\n",
        "        banned_words=None,\n",
        "        banned_weight: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.layer_idx = layer_idx\n",
        "        self.device = device\n",
        "\n",
        "        # Reference states: [T_ref, d_model]\n",
        "        self.register_buffer(\"H_ref\", H_ref.to(device))\n",
        "\n",
        "        self.prompt_length = prompt_length\n",
        "        self.banned_weight = banned_weight\n",
        "\n",
        "        # Initialize soft prompt with small random noise\n",
        "        self.soft_prompt = nn.Parameter(\n",
        "            torch.randn(prompt_length, d_model, device=device) * 0.02\n",
        "        )\n",
        "\n",
        "        # Precompute phrase-level embeddings for banned words (if any)\n",
        "        token_embed = self.model.get_input_embeddings()\n",
        "        phrase_vecs = []\n",
        "        if banned_words:\n",
        "            for w in banned_words:\n",
        "                ids = tokenizer(w, add_special_tokens=False).input_ids\n",
        "                if not ids:\n",
        "                    continue\n",
        "                # average subtoken embeddings for this word/phrase\n",
        "                emb = token_embed.weight[ids].detach().to(device).mean(dim=0)\n",
        "                phrase_vecs.append(emb)\n",
        "\n",
        "        if phrase_vecs:\n",
        "            banned_emb = torch.stack(phrase_vecs, dim=0)   # [n_banned, d]\n",
        "            banned_emb = banned_emb / (\n",
        "                banned_emb.norm(dim=-1, keepdim=True) + 1e-8\n",
        "            )\n",
        "            self.register_buffer(\"banned_emb\", banned_emb)\n",
        "        else:\n",
        "            self.banned_emb = None\n",
        "\n",
        "    def forward(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Run the model with the soft prompt and compute:\n",
        "        - H_soft : [T, d] hidden states at the target layer\n",
        "        - loss   : MSE between normalized H_soft and H_ref (tokenwise)\n",
        "                   + banned-word penalty\n",
        "        \"\"\"\n",
        "        # [1, T, d_model]\n",
        "        inputs_embeds = self.soft_prompt.unsqueeze(0)\n",
        "\n",
        "        outputs = self.model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states\n",
        "        hs_layer = hidden_states[self.layer_idx]  # [1, T, d_model]\n",
        "        H_soft = hs_layer[0]                      # [T, d_model]\n",
        "\n",
        "        # If lengths differ for some reason, crop to min length\n",
        "        T_soft = H_soft.shape[0]\n",
        "        T_ref = self.H_ref.shape[0]\n",
        "        T = min(T_soft, T_ref)\n",
        "\n",
        "        H_soft_sel = H_soft[:T]       # [T, d]\n",
        "        H_ref_sel = self.H_ref[:T]    # [T, d]\n",
        "\n",
        "        # Normalize each token vector before comparing (cosine-ish)\n",
        "        H_soft_norm = H_soft_sel / (H_soft_sel.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "        H_ref_norm = H_ref_sel / (H_ref_sel.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # Base loss: MSE between normalized states\n",
        "        mse_loss = ((H_soft_norm - H_ref_norm) ** 2).mean()\n",
        "\n",
        "        # Banned-word penalty: avoid average hidden state aligning with banned embeddings\n",
        "        penalty = 0.0\n",
        "        if self.banned_emb is not None and self.banned_weight > 0.0:\n",
        "            # average over tokens\n",
        "            h_mean = H_soft_norm.mean(dim=0)                 # [d]\n",
        "            sim_to_banned = torch.matmul(self.banned_emb, h_mean)  # [n_banned]\n",
        "            # penalize large |similarity|^2\n",
        "            penalty = self.banned_weight * (sim_to_banned.pow(2).mean())\n",
        "\n",
        "        loss = mse_loss + penalty\n",
        "\n",
        "        return H_soft, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dW3aeZnxAoXg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW3aeZnxAoXg",
        "outputId": "e3441b00-41df-4646-ea63-f43ab15d7bb8"
      },
      "outputs": [],
      "source": [
        "T_ref"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2fc9c47e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2fc9c47e",
        "outputId": "1dfd49e8-d446-451f-8c62-a5d78b4fe305"
      },
      "outputs": [],
      "source": [
        "prompt_length = T_ref  # try same length as reference prompt\n",
        "matcher = SoftPromptMatcher(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    d_model=d_model,\n",
        "    prompt_length=prompt_length,\n",
        "    layer_idx=layer_idx,\n",
        "    H_ref=H_ref,\n",
        "    device=device,\n",
        "    banned_words=banned_words,\n",
        "    banned_weight=banned_weight,\n",
        ").to(device)\n",
        "\n",
        "steps = 1\n",
        "lr = 1e-2\n",
        "\n",
        "optimizer = torch.optim.Adam([matcher.soft_prompt], lr=lr)\n",
        "\n",
        "history = []\n",
        "\n",
        "for step in range(steps):\n",
        "    optimizer.zero_grad()\n",
        "    _, loss = matcher()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    history.append(loss.item())\n",
        "\n",
        "    if step % max(1, steps // 10) == 0 or step == steps - 1:\n",
        "        print(f\"[step {step:4d}] loss={loss.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aaf290fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaf290fb",
        "outputId": "cc438bf4-3038-49d9-d99e-98c596fe2cf2"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def project_soft_prompt_to_tokens(\n",
        "    soft_prompt: torch.Tensor,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    top_k: int = 1,\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    For each soft embedding vector, pick the nearest token embedding(s) by cosine similarity.\n",
        "\n",
        "    soft_prompt: [T, d]\n",
        "    Returns: list of T token IDs.\n",
        "    \"\"\"\n",
        "    token_embed = model.get_input_embeddings().weight  # [V, d]\n",
        "    device = token_embed.device\n",
        "\n",
        "    soft_prompt = soft_prompt.to(device)\n",
        "\n",
        "    V, d = token_embed.shape\n",
        "    T = soft_prompt.shape[0]\n",
        "\n",
        "    # Normalize\n",
        "    token_embed_norm = token_embed / (token_embed.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "    soft_norm = soft_prompt / (soft_prompt.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "    token_ids = []\n",
        "\n",
        "    for i in range(T):\n",
        "        e = soft_norm[i]  # [d]\n",
        "        sims = torch.matmul(token_embed_norm, e)  # [V]\n",
        "        if top_k == 1:\n",
        "            best_id = torch.argmax(sims).item()\n",
        "            token_ids.append(best_id)\n",
        "        else:\n",
        "            topk = torch.topk(sims, k=top_k)\n",
        "            best_id = topk.indices[0].item()\n",
        "            token_ids.append(best_id)\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "soft_prompt_learned = matcher.soft_prompt.detach().cpu()\n",
        "alt_token_ids = project_soft_prompt_to_tokens(\n",
        "    soft_prompt_learned, model, tokenizer, top_k=1\n",
        ")\n",
        "alt_prompt_str = tokenizer.decode(alt_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Reference prompt:\", repr(reference_prompt))\n",
        "print(\"Alternative prompt:\", repr(alt_prompt_str))\n",
        "print(\"Are they exactly equal?\", reference_prompt == alt_prompt_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "92ba9dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ba9dae",
        "outputId": "8c7f6b88-3458-490a-d9e1-806e64c60819"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def hidden_sequence_for_prompt(\n",
        "    prompt: str,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    layer_idx: int,\n",
        "    device: torch.device,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Get hidden states [T, d] at layer_idx for a text prompt.\n",
        "    \"\"\"\n",
        "    enc = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        output_hidden_states=True,\n",
        "    )\n",
        "    hs_layer = outputs.hidden_states[layer_idx][0]  # [T, d]\n",
        "    return hs_layer.detach()\n",
        "\n",
        "\n",
        "def sequence_cosine_similarity(\n",
        "    H_a: torch.Tensor,\n",
        "    H_b: torch.Tensor,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Average cosine similarity over token positions, after aligning lengths.\n",
        "    \"\"\"\n",
        "    T = min(H_a.shape[0], H_b.shape[0])\n",
        "    A = H_a[:T]\n",
        "    B = H_b[:T]\n",
        "\n",
        "    A_norm = A / (A.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "    B_norm = B / (B.norm(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "    cos_t = (A_norm * B_norm).sum(dim=-1)  # [T]\n",
        "    return cos_t.mean().item()\n",
        "\n",
        "\n",
        "# 1) Reference hidden states (already have H_ref)\n",
        "H_ref_eval = H_ref.to(device)\n",
        "\n",
        "# 2) Continuous soft prompt hidden states\n",
        "with torch.no_grad():\n",
        "    H_soft_continuous, _ = matcher()     # [T_soft, d]\n",
        "    H_soft_continuous = H_soft_continuous.detach()\n",
        "\n",
        "# 3) Discrete alternative prompt hidden states\n",
        "H_alt_discrete = hidden_sequence_for_prompt(\n",
        "    alt_prompt_str,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    layer_idx=layer_idx,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "sim_ref_soft = sequence_cosine_similarity(H_ref_eval, H_soft_continuous)\n",
        "sim_ref_alt = sequence_cosine_similarity(H_ref_eval, H_alt_discrete)\n",
        "\n",
        "print(\"Average cosine similarity with reference states:\")\n",
        "print(f\"  Continuous soft prompt: {sim_ref_soft:.4f}\")\n",
        "print(f\"  Discrete alternative prompt: {sim_ref_alt:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c31a11ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c31a11ee",
        "outputId": "7e4a7f14-d55f-4e58-f7b0-111ab68b952e"
      },
      "outputs": [],
      "source": [
        "random_prompt = \"This is a totally random unrelated sentence about weather.\"\n",
        "\n",
        "H_random = hidden_sequence_for_prompt(\n",
        "    random_prompt,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    layer_idx=layer_idx,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "sim_ref_random = sequence_cosine_similarity(H_ref_eval, H_random)\n",
        "\n",
        "print(\"Random baseline prompt:\", repr(random_prompt))\n",
        "print(f\"  Similarity(ref, random)   = {sim_ref_random:.4f}\")\n",
        "print(f\"  Similarity(ref, alt)      = {sim_ref_alt:.4f}\")\n",
        "print(f\"  Similarity(ref, soft)     = {sim_ref_soft:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "vhEVFZJbPhc-",
      "metadata": {
        "id": "vhEVFZJbPhc-"
      },
      "outputs": [],
      "source": [
        "# If you already defined generate_from_prompt earlier, you can delete this block.\n",
        "@torch.no_grad()\n",
        "def generate_from_prompt_local(\n",
        "    prompt: str,\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    device: torch.device,\n",
        "    max_new_tokens: int = 80,\n",
        "    temperature: float = 0.8,\n",
        "    repetition_penalty: float = 1.1,\n",
        "    no_repeat_ngram_size: int = 3,\n",
        ") -> str:\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out_ids = model.generate(\n",
        "        **enc,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "    )\n",
        "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def project_state_to_tokens():\n",
        "    with torch.no_grad():   \n",
        "        soft_prompt_learned = matcher.soft_prompt.detach().cpu()\n",
        "        alt_token_ids = project_soft_prompt_to_tokens(\n",
        "            soft_prompt_learned,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            top_k=1,\n",
        "        )\n",
        "        alt_prompt_str = tokenizer.decode(alt_token_ids)\n",
        "\n",
        "    return alt_prompt_str\n",
        "\n",
        "# print(\"=== Reference prompt ===\")\n",
        "# print(\"Prompt:\", repr(reference_prompt))\n",
        "# ref_gen = generate_from_prompt_local(\n",
        "#     reference_prompt,\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     device=device,\n",
        "# )\n",
        "# print(\"Generation:\\n\", ref_gen)\n",
        "\n",
        "# print(\"\\n=== Alternative (matched) prompt ===\")\n",
        "# print(\"Prompt:\", repr(alt_prompt_str))\n",
        "# alt_gen = generate_from_prompt_local(\n",
        "#     alt_prompt_str,\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     device=device,\n",
        "# )\n",
        "# print(\"Generation:\\n\", alt_gen)\n",
        "# print(alt_gen == ref_gen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07poFvuASowI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07poFvuASowI",
        "outputId": "6c1a6a0a-1b65-4799-ef2c-995e3f3ab4a6"
      },
      "outputs": [],
      "source": [
        "# === Layer sweep with fixed seeds, early stopping, and generation ===\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "target_layers = range(0, 26, 2)\n",
        "\n",
        "steps = 1000\n",
        "lr = 1e-2\n",
        "\n",
        "n_tokens_to_generate = 80      # how many *new* tokens to generate\n",
        "lower_loss_threshold = 1e-4    # early stopping threshold\n",
        "\n",
        "layer_prompts = {}\n",
        "layer_histories = {}\n",
        "\n",
        "# Global base seed – controls everything\n",
        "base_seed = 4738\n",
        "\n",
        "for layer_idx in target_layers:\n",
        "    # ----- set seeds so this run is fully reproducible -----\n",
        "    # If you want the *same* seed for every layer, just use base_seed\n",
        "    # instead of base_seed + layer_idx.\n",
        "    layer_seed = base_seed + layer_idx\n",
        "    reached_original_prompt = False\n",
        "\n",
        "    random.seed(layer_seed)\n",
        "    np.random.seed(layer_seed)\n",
        "    torch.manual_seed(layer_seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(layer_seed)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(f\" Optimizing soft prompt for layer {layer_idx} (seed={layer_seed})\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # 1) Reference hidden states for this layer\n",
        "    H_ref_layer = get_hidden_states_for_prompt(\n",
        "        reference_prompt,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        layer_idx=layer_idx,\n",
        "        device=device,\n",
        "    )\n",
        "    T_ref, d_model = H_ref_layer.shape\n",
        "    print(f\"Layer {layer_idx}: H_ref shape = {H_ref_layer.shape} (T={T_ref}, d={d_model})\")\n",
        "\n",
        "    # 2) Build matcher for this layer\n",
        "    matcher = SoftPromptMatcher(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        d_model=d_model,\n",
        "        prompt_length=prompt_length,\n",
        "        layer_idx=layer_idx,\n",
        "        H_ref=H_ref_layer,\n",
        "        device=device,\n",
        "        banned_words=banned_words,\n",
        "        banned_weight=banned_weight,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam([matcher.soft_prompt], lr=lr)\n",
        "    history = []\n",
        "\n",
        "    # 3) Optimize with early stopping\n",
        "    for step in range(steps):\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = matcher()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_value = loss.item()\n",
        "        history.append(loss_value)\n",
        "\n",
        "        current_prompt = project_state_to_tokens()\n",
        "\n",
        "        if step % max(1, steps // 20) == 0 or step == steps - 1:\n",
        "            optimizer.zero_grad()\n",
        "            print(f\"[layer {layer_idx:2d} | step {step:4d}] loss={loss_value:.6f} | {current_prompt}\")\n",
        "\n",
        "        if loss_value < lower_loss_threshold:\n",
        "            print(\n",
        "                f\"--Early stopping on step {step}--\\n\"\n",
        "                f\"Loss of {loss_value:.6f} < {lower_loss_threshold}\"\n",
        "            )\n",
        "            break\n",
        "\n",
        "        elif current_prompt == \"<bos>Talk about cats.\":\n",
        "            print(\n",
        "                f\"--Early stopping on step {step}--\\n\"\n",
        "                f\"Model reached original prompt\"\n",
        "            )\n",
        "            reached_original_prompt = True\n",
        "            break\n",
        "\n",
        "    if reached_original_prompt:\n",
        "        continue\n",
        "\n",
        "    layer_histories[layer_idx] = history\n",
        "\n",
        "    # 4) Project learned soft prompt to discrete tokens and decode\n",
        "    with torch.no_grad():\n",
        "        soft_prompt_learned = matcher.soft_prompt.detach().cpu()\n",
        "        alt_token_ids = project_soft_prompt_to_tokens(\n",
        "            soft_prompt_learned,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            top_k=1,\n",
        "        )\n",
        "        alt_prompt_str = tokenizer.decode(alt_token_ids)\n",
        "\n",
        "    layer_prompts[layer_idx] = alt_prompt_str\n",
        "\n",
        "    print(f\"\\nLayer {layer_idx} alternative prompt:\")\n",
        "    print(repr(alt_prompt_str))\n",
        "\n",
        "    # 5) Generate n tokens after this alternative starter prompt\n",
        "    # Re-seed before generation to keep sampling reproducible as well\n",
        "    random.seed(layer_seed)\n",
        "    np.random.seed(layer_seed)\n",
        "    torch.manual_seed(layer_seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(layer_seed)\n",
        "\n",
        "    alt_gen = generate_from_prompt_local(\n",
        "        alt_prompt_str,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        device=device,\n",
        "        max_new_tokens=n_tokens_to_generate,\n",
        "    )\n",
        "    print(f\"\\nSample generation from layer {layer_idx} prompt \"\n",
        "          f\"(first {n_tokens_to_generate} new tokens):\")\n",
        "    print(alt_gen)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a8c935",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MATS-10.0 (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15aa2882a6974dc1a91816fcd2eee848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe469d4818f44cc97ee89b91daf1acb",
            "placeholder": "​",
            "style": "IPY_MODEL_b380e56f51bf42d5bd3e14d2377d5581",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "197f1f21b12342c3afca9309d1507270": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15aa2882a6974dc1a91816fcd2eee848",
              "IPY_MODEL_4c6ee817b10342b891812607311d5b19",
              "IPY_MODEL_57921850c2424f7a95e18f6df0e6ed3a"
            ],
            "layout": "IPY_MODEL_bb0b698b75f14b22992f19647f18ae12"
          }
        },
        "4c6ee817b10342b891812607311d5b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e441a7153bf489080044e1c2002ed51",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8f7564ad2ff44809bd3ac3a7320855b",
            "value": 2
          }
        },
        "57921850c2424f7a95e18f6df0e6ed3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_656abb4de5e048fa858bb118b0f41213",
            "placeholder": "​",
            "style": "IPY_MODEL_9429a5ca16a3463a90996bf43f2dde18",
            "value": " 2/2 [00:01&lt;00:00,  1.46s/it]"
          }
        },
        "656abb4de5e048fa858bb118b0f41213": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e441a7153bf489080044e1c2002ed51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9429a5ca16a3463a90996bf43f2dde18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b380e56f51bf42d5bd3e14d2377d5581": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8f7564ad2ff44809bd3ac3a7320855b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb0b698b75f14b22992f19647f18ae12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe469d4818f44cc97ee89b91daf1acb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
